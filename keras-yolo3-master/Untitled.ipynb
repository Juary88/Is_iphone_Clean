{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'VOCdevkit/VOC2007/JPEGImages/Porsche.C39WQ00XK946_4S5G2_008686.XRAY_Up.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "2019-01-03 06:14:32.238224: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-01-03 06:14:33.014075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: \n",
      "name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.8225\n",
      "pciBusID: 0000:01:00.0\n",
      "totalMemory: 7.93GiB freeMemory: 7.81GiB\n",
      "2019-01-03 06:14:33.164043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 1 with properties: \n",
      "name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.8225\n",
      "pciBusID: 0000:02:00.0\n",
      "totalMemory: 7.93GiB freeMemory: 7.81GiB\n",
      "2019-01-03 06:14:33.165360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0, 1\n",
      "2019-01-03 06:14:33.730035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-01-03 06:14:33.730092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 1 \n",
      "2019-01-03 06:14:33.730106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N Y \n",
      "2019-01-03 06:14:33.730133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1:   Y N \n",
      "2019-01-03 06:14:33.730485: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7541 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "2019-01-03 06:14:33.773014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7541 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)\n",
      "Create YOLOv3 model with 9 anchors and 1 classes.\n",
      "Train on 84 samples, val on 9 samples, with batch size 10.\n",
      "Epoch 1/800\n",
      "2019-01-03 06:15:14.574810: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.16GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2019-01-03 06:15:14.574904: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.19GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "8/8 [==============================] - 27s 3s/step - loss: 4565.7813 - val_loss: 35840148.0000\n",
      "Epoch 2/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 988.4171 - val_loss: 38069428.0000\n",
      "Epoch 3/800\n",
      "8/8 [==============================] - 6s 798ms/step - loss: 532.2477 - val_loss: 670655.3750\n",
      "Epoch 4/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 343.6899 - val_loss: 34624.2812\n",
      "Epoch 5/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 254.0390 - val_loss: 8920.4062\n",
      "Epoch 6/800\n",
      "8/8 [==============================] - 6s 806ms/step - loss: 204.7813 - val_loss: 281.2048\n",
      "Epoch 7/800\n",
      "8/8 [==============================] - 6s 796ms/step - loss: 164.8395 - val_loss: 191.4999\n",
      "Epoch 8/800\n",
      "8/8 [==============================] - 6s 796ms/step - loss: 140.4315 - val_loss: 174.8512\n",
      "Epoch 9/800\n",
      "8/8 [==============================] - 6s 783ms/step - loss: 127.3718 - val_loss: 159.2766\n",
      "Epoch 10/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 112.3500 - val_loss: 141.3860\n",
      "Epoch 11/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 101.0219 - val_loss: 122.6908\n",
      "Epoch 12/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 94.0178 - val_loss: 113.1684\n",
      "Epoch 13/800\n",
      "8/8 [==============================] - 6s 786ms/step - loss: 85.8993 - val_loss: 105.0793\n",
      "Epoch 14/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 79.0682 - val_loss: 95.5934\n",
      "Epoch 15/800\n",
      "8/8 [==============================] - 6s 798ms/step - loss: 73.2538 - val_loss: 90.0620\n",
      "Epoch 16/800\n",
      "8/8 [==============================] - 6s 808ms/step - loss: 71.3109 - val_loss: 97.2149\n",
      "Epoch 17/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 66.9642 - val_loss: 78.0055\n",
      "Epoch 18/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 63.8735 - val_loss: 76.4936\n",
      "Epoch 19/800\n",
      "8/8 [==============================] - 6s 811ms/step - loss: 60.5928 - val_loss: 69.7647\n",
      "Epoch 20/800\n",
      "8/8 [==============================] - 6s 806ms/step - loss: 61.1350 - val_loss: 71.4270\n",
      "Epoch 21/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 58.5750 - val_loss: 59.6902\n",
      "Epoch 22/800\n",
      "8/8 [==============================] - 6s 797ms/step - loss: 56.0381 - val_loss: 55.0924\n",
      "Epoch 23/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 53.0468 - val_loss: 54.5623\n",
      "Epoch 24/800\n",
      "8/8 [==============================] - 6s 797ms/step - loss: 50.5056 - val_loss: 50.5788\n",
      "Epoch 25/800\n",
      "8/8 [==============================] - 6s 794ms/step - loss: 48.9148 - val_loss: 50.3760\n",
      "Epoch 26/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 46.1477 - val_loss: 51.9722\n",
      "Epoch 27/800\n",
      "8/8 [==============================] - 6s 807ms/step - loss: 44.9425 - val_loss: 48.6707\n",
      "Epoch 28/800\n",
      "8/8 [==============================] - 6s 797ms/step - loss: 43.7085 - val_loss: 47.4926\n",
      "Epoch 29/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 43.5978 - val_loss: 45.0753\n",
      "Epoch 30/800\n",
      "8/8 [==============================] - 6s 783ms/step - loss: 42.9831 - val_loss: 43.0232\n",
      "Epoch 31/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 41.3062 - val_loss: 42.0176\n",
      "Epoch 32/800\n",
      "8/8 [==============================] - 6s 809ms/step - loss: 40.6396 - val_loss: 44.1867\n",
      "Epoch 33/800\n",
      "8/8 [==============================] - 6s 808ms/step - loss: 39.0904 - val_loss: 41.3637\n",
      "Epoch 34/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 38.1917 - val_loss: 39.5696\n",
      "Epoch 35/800\n",
      "8/8 [==============================] - 6s 795ms/step - loss: 37.3270 - val_loss: 39.0839\n",
      "Epoch 36/800\n",
      "8/8 [==============================] - 6s 810ms/step - loss: 36.1667 - val_loss: 37.2343\n",
      "Epoch 37/800\n",
      "8/8 [==============================] - 6s 807ms/step - loss: 35.6828 - val_loss: 36.6125\n",
      "Epoch 38/800\n",
      "8/8 [==============================] - 6s 794ms/step - loss: 34.9872 - val_loss: 36.6427\n",
      "Epoch 39/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 35.3577 - val_loss: 35.4961\n",
      "Epoch 40/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 34.8945 - val_loss: 2966.6179\n",
      "Epoch 41/800\n",
      "8/8 [==============================] - 6s 806ms/step - loss: 34.6126 - val_loss: 803.0034\n",
      "Epoch 42/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 34.4149 - val_loss: 36.9116\n",
      "Epoch 43/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 33.0611 - val_loss: 34.7311\n",
      "Epoch 44/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 34.6952 - val_loss: 33.9244\n",
      "Epoch 45/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 32.4518 - val_loss: 129.2840\n",
      "Epoch 46/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 31.9906 - val_loss: 34.7817\n",
      "Epoch 47/800\n",
      "8/8 [==============================] - 6s 794ms/step - loss: 39.5606 - val_loss: 33.8245\n",
      "Epoch 48/800\n",
      "8/8 [==============================] - 6s 808ms/step - loss: 33.2725 - val_loss: 31.5107\n",
      "Epoch 49/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 32.4557 - val_loss: 31.6929\n",
      "Epoch 50/800\n",
      "8/8 [==============================] - 6s 786ms/step - loss: 31.3545 - val_loss: 37.9156\n",
      "Epoch 51/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 30.9911 - val_loss: 32.0371\n",
      "Epoch 52/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 31.0041 - val_loss: 31.2321\n",
      "Epoch 53/800\n",
      "8/8 [==============================] - 6s 795ms/step - loss: 30.0722 - val_loss: 34.5888\n",
      "Epoch 54/800\n",
      "8/8 [==============================] - 6s 806ms/step - loss: 30.2792 - val_loss: 30.4191\n",
      "Epoch 55/800\n",
      "8/8 [==============================] - 6s 808ms/step - loss: 162.9196 - val_loss: 1954.7294\n",
      "Epoch 56/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 6s 805ms/step - loss: 41.2451 - val_loss: 4552811008.0000\n",
      "Epoch 57/800\n",
      "8/8 [==============================] - 6s 790ms/step - loss: 36.6883 - val_loss: 3326357248.0000\n",
      "Epoch 58/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 36.7926 - val_loss: 1295905792.0000\n",
      "Epoch 59/800\n",
      "8/8 [==============================] - 6s 789ms/step - loss: 33.0754 - val_loss: 2480709.7500\n",
      "Epoch 60/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 31.9641 - val_loss: 5612111.0000\n",
      "Epoch 61/800\n",
      "8/8 [==============================] - 6s 798ms/step - loss: 32.0673 - val_loss: 431396.3438\n",
      "Epoch 62/800\n",
      "8/8 [==============================] - 6s 797ms/step - loss: 30.3694 - val_loss: 225372.4531\n",
      "Epoch 63/800\n",
      "8/8 [==============================] - 6s 794ms/step - loss: 41.3058 - val_loss: 568844.8750\n",
      "Epoch 64/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 29.8369 - val_loss: 29772.3691\n",
      "Epoch 65/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 28.8241 - val_loss: 2662.4724\n",
      "Epoch 66/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 28.5562 - val_loss: 43.0592\n",
      "Epoch 67/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 28.5228 - val_loss: 156.6174\n",
      "Epoch 68/800\n",
      "8/8 [==============================] - 6s 796ms/step - loss: 29.2728 - val_loss: 744.8647\n",
      "Epoch 69/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 28.5328 - val_loss: 211.0479\n",
      "Epoch 70/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 27.8858 - val_loss: 414.8059\n",
      "Epoch 71/800\n",
      "8/8 [==============================] - 6s 809ms/step - loss: 27.8487 - val_loss: 686.9531\n",
      "Epoch 72/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 27.3209 - val_loss: 69.7063\n",
      "Epoch 73/800\n",
      "8/8 [==============================] - 6s 808ms/step - loss: 27.3121 - val_loss: 28.6286\n",
      "Epoch 74/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 26.6646 - val_loss: 27.8669\n",
      "Epoch 75/800\n",
      "8/8 [==============================] - 6s 786ms/step - loss: 27.0390 - val_loss: 29.8253\n",
      "Epoch 76/800\n",
      "8/8 [==============================] - 6s 810ms/step - loss: 26.9529 - val_loss: 27.7168\n",
      "Epoch 77/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 26.7031 - val_loss: 57.1588\n",
      "Epoch 78/800\n",
      "8/8 [==============================] - 7s 813ms/step - loss: 26.4158 - val_loss: 27.6160\n",
      "Epoch 79/800\n",
      "8/8 [==============================] - 6s 807ms/step - loss: 26.1352 - val_loss: 27.6015\n",
      "Epoch 80/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 26.0884 - val_loss: 26.8362\n",
      "Epoch 81/800\n",
      "8/8 [==============================] - 6s 809ms/step - loss: 25.8776 - val_loss: 28.1175\n",
      "Epoch 82/800\n",
      "8/8 [==============================] - 6s 807ms/step - loss: 25.9182 - val_loss: 27.0122\n",
      "Epoch 83/800\n",
      "8/8 [==============================] - 6s 806ms/step - loss: 25.4825 - val_loss: 26.0845\n",
      "Epoch 84/800\n",
      "8/8 [==============================] - 6s 787ms/step - loss: 25.6239 - val_loss: 25.8302\n",
      "Epoch 85/800\n",
      "8/8 [==============================] - 6s 794ms/step - loss: 25.3785 - val_loss: 26.8243\n",
      "Epoch 86/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 25.4536 - val_loss: 25.6151\n",
      "Epoch 87/800\n",
      "8/8 [==============================] - 6s 795ms/step - loss: 24.7854 - val_loss: 26.1936\n",
      "Epoch 88/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 25.3231 - val_loss: 25.1676\n",
      "Epoch 89/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 25.1861 - val_loss: 25.1256\n",
      "Epoch 90/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 24.8906 - val_loss: 27.9233\n",
      "Epoch 91/800\n",
      "8/8 [==============================] - 6s 809ms/step - loss: 24.8460 - val_loss: 25.5464\n",
      "Epoch 92/800\n",
      "8/8 [==============================] - 6s 796ms/step - loss: 24.6030 - val_loss: 25.0887\n",
      "Epoch 93/800\n",
      "8/8 [==============================] - 6s 788ms/step - loss: 24.6063 - val_loss: 25.3527\n",
      "Epoch 94/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 24.5248 - val_loss: 24.7056\n",
      "Epoch 95/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 24.2969 - val_loss: 25.1330\n",
      "Epoch 96/800\n",
      "8/8 [==============================] - 6s 790ms/step - loss: 24.1866 - val_loss: 24.9820\n",
      "Epoch 97/800\n",
      "8/8 [==============================] - 6s 806ms/step - loss: 24.2187 - val_loss: 24.6996\n",
      "Epoch 98/800\n",
      "8/8 [==============================] - 6s 798ms/step - loss: 23.7557 - val_loss: 25.2976\n",
      "Epoch 99/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 24.2357 - val_loss: 23.9093\n",
      "Epoch 100/800\n",
      "8/8 [==============================] - 6s 793ms/step - loss: 24.2645 - val_loss: 24.3348\n",
      "Epoch 101/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 24.1457 - val_loss: 23.8028\n",
      "Epoch 102/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 23.5430 - val_loss: 23.6517\n",
      "Epoch 103/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 23.7233 - val_loss: 24.1282\n",
      "Epoch 104/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 23.5902 - val_loss: 23.9494\n",
      "Epoch 105/800\n",
      "8/8 [==============================] - 6s 806ms/step - loss: 23.7300 - val_loss: 22.7424\n",
      "Epoch 106/800\n",
      "8/8 [==============================] - 6s 806ms/step - loss: 23.4032 - val_loss: 23.3188\n",
      "Epoch 107/800\n",
      "8/8 [==============================] - 6s 796ms/step - loss: 23.7476 - val_loss: 24.3435\n",
      "Epoch 108/800\n",
      "8/8 [==============================] - 6s 810ms/step - loss: 23.6639 - val_loss: 22.6091\n",
      "Epoch 109/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 22.9622 - val_loss: 23.7852\n",
      "Epoch 110/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 23.5031 - val_loss: 24.0179\n",
      "Epoch 111/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 23.6760 - val_loss: 21.7447\n",
      "Epoch 112/800\n",
      "8/8 [==============================] - 6s 807ms/step - loss: 23.3320 - val_loss: 23.0977\n",
      "Epoch 113/800\n",
      "8/8 [==============================] - 6s 796ms/step - loss: 22.8919 - val_loss: 24.1788\n",
      "Epoch 114/800\n",
      "8/8 [==============================] - 6s 786ms/step - loss: 23.7075 - val_loss: 23.6742\n",
      "Epoch 115/800\n",
      "8/8 [==============================] - 6s 798ms/step - loss: 23.2510 - val_loss: 22.6764\n",
      "Epoch 116/800\n",
      "8/8 [==============================] - 6s 783ms/step - loss: 22.5468 - val_loss: 23.1857\n",
      "Epoch 117/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 23.2825 - val_loss: 22.7110\n",
      "Epoch 118/800\n",
      "8/8 [==============================] - 6s 797ms/step - loss: 22.7586 - val_loss: 24.1528\n",
      "Epoch 119/800\n",
      "8/8 [==============================] - 6s 807ms/step - loss: 22.9058 - val_loss: 25.1239\n",
      "Epoch 120/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 23.0243 - val_loss: 23.8181\n",
      "Epoch 121/800\n",
      "8/8 [==============================] - 6s 810ms/step - loss: 22.7559 - val_loss: 22.1961\n",
      "Epoch 122/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 22.9459 - val_loss: 22.2208\n",
      "Epoch 123/800\n",
      "8/8 [==============================] - 6s 791ms/step - loss: 22.7807 - val_loss: 23.0932\n",
      "Epoch 124/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 22.6815 - val_loss: 23.4973\n",
      "Epoch 125/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 22.6016 - val_loss: 22.6946\n",
      "Epoch 126/800\n",
      "8/8 [==============================] - 6s 794ms/step - loss: 22.1948 - val_loss: 22.9721\n",
      "Epoch 127/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 22.0021 - val_loss: 21.7281\n",
      "Epoch 128/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 22.5250 - val_loss: 23.5824\n",
      "Epoch 129/800\n",
      "8/8 [==============================] - 6s 789ms/step - loss: 22.1452 - val_loss: 22.3688\n",
      "Epoch 130/800\n",
      "8/8 [==============================] - 6s 789ms/step - loss: 22.0711 - val_loss: 22.5801\n",
      "Epoch 131/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 21.8114 - val_loss: 21.7784\n",
      "Epoch 132/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 21.6084 - val_loss: 22.5169\n",
      "Epoch 133/800\n",
      "8/8 [==============================] - 6s 797ms/step - loss: 21.8264 - val_loss: 23.4756\n",
      "Epoch 134/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 21.8327 - val_loss: 22.2799\n",
      "Epoch 135/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 6s 800ms/step - loss: 21.6901 - val_loss: 21.5119\n",
      "Epoch 136/800\n",
      "8/8 [==============================] - 6s 798ms/step - loss: 21.7810 - val_loss: 22.4351\n",
      "Epoch 137/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 21.4154 - val_loss: 21.0401\n",
      "Epoch 138/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 21.2125 - val_loss: 24.3909\n",
      "Epoch 139/800\n",
      "8/8 [==============================] - 6s 809ms/step - loss: 21.0916 - val_loss: 23.0599\n",
      "Epoch 140/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 21.4494 - val_loss: 22.5448\n",
      "Epoch 141/800\n",
      "8/8 [==============================] - 6s 793ms/step - loss: 21.3729 - val_loss: 23.6888\n",
      "Epoch 142/800\n",
      "8/8 [==============================] - 6s 788ms/step - loss: 22.2750 - val_loss: 22.0846\n",
      "Epoch 143/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 21.7871 - val_loss: 21.4670\n",
      "Epoch 144/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 21.4077 - val_loss: 21.1551\n",
      "Epoch 145/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 21.5567 - val_loss: 21.7895\n",
      "Epoch 146/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 21.2590 - val_loss: 21.5748\n",
      "Epoch 147/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 20.9679 - val_loss: 21.0134\n",
      "Epoch 148/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 21.3567 - val_loss: 21.5679\n",
      "Epoch 149/800\n",
      "8/8 [==============================] - 6s 798ms/step - loss: 20.6553 - val_loss: 21.1976\n",
      "Epoch 150/800\n",
      "8/8 [==============================] - 6s 810ms/step - loss: 20.9032 - val_loss: 21.6396\n",
      "Epoch 151/800\n",
      "8/8 [==============================] - 6s 806ms/step - loss: 20.7498 - val_loss: 20.1355\n",
      "Epoch 152/800\n",
      "8/8 [==============================] - 6s 797ms/step - loss: 21.0946 - val_loss: 21.8859\n",
      "Epoch 153/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 21.0381 - val_loss: 21.0954\n",
      "Epoch 154/800\n",
      "8/8 [==============================] - 6s 794ms/step - loss: 20.4177 - val_loss: 22.4431\n",
      "Epoch 155/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 20.6545 - val_loss: 22.0921\n",
      "Epoch 156/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 20.8175 - val_loss: 20.8606\n",
      "Epoch 157/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 20.4828 - val_loss: 20.5857\n",
      "Epoch 158/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 20.2425 - val_loss: 19.4566\n",
      "Epoch 159/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 20.7493 - val_loss: 20.0603\n",
      "Epoch 160/800\n",
      "8/8 [==============================] - 6s 796ms/step - loss: 20.0540 - val_loss: 19.8378\n",
      "Epoch 161/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 20.3479 - val_loss: 19.7448\n",
      "Epoch 162/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 19.7630 - val_loss: 20.3791\n",
      "Epoch 163/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 19.9433 - val_loss: 19.9967\n",
      "Epoch 164/800\n",
      "8/8 [==============================] - 6s 807ms/step - loss: 20.1145 - val_loss: 19.7228\n",
      "Epoch 165/800\n",
      "8/8 [==============================] - 6s 788ms/step - loss: 19.9872 - val_loss: 22.5983\n",
      "Epoch 166/800\n",
      "8/8 [==============================] - 6s 808ms/step - loss: 19.5261 - val_loss: 21.0157\n",
      "Epoch 167/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 19.7741 - val_loss: 20.4838\n",
      "Epoch 168/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 19.7270 - val_loss: 19.3819\n",
      "Epoch 169/800\n",
      "8/8 [==============================] - 6s 808ms/step - loss: 19.2919 - val_loss: 20.9115\n",
      "Epoch 170/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 19.6385 - val_loss: 20.6801\n",
      "Epoch 171/800\n",
      "8/8 [==============================] - 6s 793ms/step - loss: 19.5041 - val_loss: 20.0594\n",
      "Epoch 172/800\n",
      "8/8 [==============================] - 6s 786ms/step - loss: 18.8732 - val_loss: 20.1173\n",
      "Epoch 173/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 19.5446 - val_loss: 20.3795\n",
      "Epoch 174/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 19.6769 - val_loss: 19.2170\n",
      "Epoch 175/800\n",
      "8/8 [==============================] - 6s 787ms/step - loss: 18.9242 - val_loss: 20.7212\n",
      "Epoch 176/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 18.7581 - val_loss: 21.4138\n",
      "Epoch 177/800\n",
      "8/8 [==============================] - 6s 788ms/step - loss: 19.3639 - val_loss: 20.6538\n",
      "Epoch 178/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 19.1228 - val_loss: 19.0213\n",
      "Epoch 179/800\n",
      "8/8 [==============================] - 6s 797ms/step - loss: 18.8903 - val_loss: 18.7969\n",
      "Epoch 180/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 18.8569 - val_loss: 19.2035\n",
      "Epoch 181/800\n",
      "8/8 [==============================] - 6s 795ms/step - loss: 19.1171 - val_loss: 20.1985\n",
      "Epoch 182/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 18.3318 - val_loss: 19.4963\n",
      "Epoch 183/800\n",
      "8/8 [==============================] - 6s 809ms/step - loss: 18.5805 - val_loss: 18.1242\n",
      "Epoch 184/800\n",
      "8/8 [==============================] - 6s 798ms/step - loss: 18.7343 - val_loss: 18.8385\n",
      "Epoch 185/800\n",
      "8/8 [==============================] - 6s 787ms/step - loss: 18.9574 - val_loss: 20.3981\n",
      "Epoch 186/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 18.6922 - val_loss: 19.6835\n",
      "Epoch 187/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 18.5645 - val_loss: 19.1441\n",
      "Epoch 188/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 18.6217 - val_loss: 19.8669\n",
      "Epoch 189/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 18.4738 - val_loss: 18.5061\n",
      "Epoch 190/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 18.3299 - val_loss: 18.6328\n",
      "Epoch 191/800\n",
      "8/8 [==============================] - 6s 791ms/step - loss: 18.8444 - val_loss: 18.0996\n",
      "Epoch 192/800\n",
      "8/8 [==============================] - 6s 792ms/step - loss: 18.3827 - val_loss: 17.5029\n",
      "Epoch 193/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 18.4802 - val_loss: 18.0896\n",
      "Epoch 194/800\n",
      "8/8 [==============================] - 6s 808ms/step - loss: 18.1988 - val_loss: 18.0839\n",
      "Epoch 195/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 18.5781 - val_loss: 18.4032\n",
      "Epoch 196/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 18.2372 - val_loss: 18.3439\n",
      "Epoch 197/800\n",
      "8/8 [==============================] - 6s 806ms/step - loss: 18.0018 - val_loss: 17.1925\n",
      "Epoch 198/800\n",
      "8/8 [==============================] - 6s 796ms/step - loss: 17.7362 - val_loss: 18.6592\n",
      "Epoch 199/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 18.1182 - val_loss: 17.1385\n",
      "Epoch 200/800\n",
      "8/8 [==============================] - 6s 797ms/step - loss: 17.7050 - val_loss: 19.2686\n",
      "Epoch 201/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 17.8542 - val_loss: 18.2093\n",
      "Epoch 202/800\n",
      "8/8 [==============================] - 6s 807ms/step - loss: 18.2345 - val_loss: 18.1432\n",
      "Epoch 203/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 18.6793 - val_loss: 17.0074\n",
      "Epoch 204/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 17.9305 - val_loss: 18.6506\n",
      "Epoch 205/800\n",
      "8/8 [==============================] - 6s 781ms/step - loss: 18.0811 - val_loss: 18.2335\n",
      "Epoch 206/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 17.9135 - val_loss: 16.9428\n",
      "Epoch 207/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 17.4639 - val_loss: 17.1878\n",
      "Epoch 208/800\n",
      "8/8 [==============================] - 6s 806ms/step - loss: 17.4849 - val_loss: 15.8789\n",
      "Epoch 209/800\n",
      "8/8 [==============================] - 6s 791ms/step - loss: 17.5955 - val_loss: 16.6103\n",
      "Epoch 210/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 16.9640 - val_loss: 16.6156\n",
      "Epoch 211/800\n",
      "8/8 [==============================] - 6s 807ms/step - loss: 17.1221 - val_loss: 18.1695\n",
      "Epoch 212/800\n",
      "8/8 [==============================] - 6s 796ms/step - loss: 18.0650 - val_loss: 17.3149\n",
      "Epoch 213/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 17.5955 - val_loss: 16.5288\n",
      "Epoch 214/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 6s 789ms/step - loss: 17.5455 - val_loss: 16.3351\n",
      "Epoch 215/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 17.2587 - val_loss: 17.3034\n",
      "Epoch 216/800\n",
      "8/8 [==============================] - 6s 806ms/step - loss: 17.2062 - val_loss: 18.4967\n",
      "Epoch 217/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 16.9775 - val_loss: 15.9796\n",
      "Epoch 218/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 16.7833 - val_loss: 18.1915\n",
      "Epoch 219/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 16.9461 - val_loss: 16.5231\n",
      "Epoch 220/800\n",
      "8/8 [==============================] - 6s 786ms/step - loss: 17.4530 - val_loss: 16.6270\n",
      "Epoch 221/800\n",
      "8/8 [==============================] - 6s 792ms/step - loss: 17.2488 - val_loss: 16.7176\n",
      "Epoch 222/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 17.2741 - val_loss: 18.3415\n",
      "Epoch 223/800\n",
      "8/8 [==============================] - 6s 807ms/step - loss: 16.8047 - val_loss: 16.9660\n",
      "Epoch 224/800\n",
      "8/8 [==============================] - 6s 797ms/step - loss: 17.9657 - val_loss: 16.1768\n",
      "Epoch 225/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 16.5467 - val_loss: 16.6443\n",
      "Epoch 226/800\n",
      "8/8 [==============================] - 6s 785ms/step - loss: 17.0361 - val_loss: 16.5805\n",
      "Epoch 227/800\n",
      "8/8 [==============================] - 6s 806ms/step - loss: 16.7415 - val_loss: 17.8536\n",
      "Epoch 228/800\n",
      "8/8 [==============================] - 6s 797ms/step - loss: 16.5120 - val_loss: 16.4027\n",
      "Epoch 229/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 16.6138 - val_loss: 17.1650\n",
      "Epoch 230/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 16.5973 - val_loss: 15.8572\n",
      "Epoch 231/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 16.2151 - val_loss: 16.0889\n",
      "Epoch 232/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 17.3956 - val_loss: 15.6666\n",
      "Epoch 233/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 16.7176 - val_loss: 16.7602\n",
      "Epoch 234/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 16.9027 - val_loss: 16.5480\n",
      "Epoch 235/800\n",
      "8/8 [==============================] - 6s 798ms/step - loss: 16.6775 - val_loss: 18.2649\n",
      "Epoch 236/800\n",
      "8/8 [==============================] - 6s 806ms/step - loss: 16.7636 - val_loss: 17.5714\n",
      "Epoch 237/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 16.7811 - val_loss: 18.4060\n",
      "Epoch 238/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 16.6193 - val_loss: 16.2743\n",
      "Epoch 239/800\n",
      "8/8 [==============================] - 6s 807ms/step - loss: 16.4075 - val_loss: 16.5605\n",
      "Epoch 240/800\n",
      "8/8 [==============================] - 6s 798ms/step - loss: 16.3802 - val_loss: 15.9546\n",
      "Epoch 241/800\n",
      "8/8 [==============================] - 6s 793ms/step - loss: 16.4325 - val_loss: 15.2763\n",
      "Epoch 242/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 16.0775 - val_loss: 15.6708\n",
      "Epoch 243/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 16.0257 - val_loss: 17.2871\n",
      "Epoch 244/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 16.1686 - val_loss: 15.7868\n",
      "Epoch 245/800\n",
      "8/8 [==============================] - 6s 807ms/step - loss: 16.5170 - val_loss: 15.6563\n",
      "Epoch 246/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 16.4873 - val_loss: 15.9694\n",
      "Epoch 247/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 16.1314 - val_loss: 15.4348\n",
      "Epoch 248/800\n",
      "8/8 [==============================] - 6s 790ms/step - loss: 16.2847 - val_loss: 14.7014\n",
      "Epoch 249/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 16.0470 - val_loss: 17.5383\n",
      "Epoch 250/800\n",
      "8/8 [==============================] - 6s 810ms/step - loss: 16.2862 - val_loss: 14.8977\n",
      "Epoch 251/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 15.2826 - val_loss: 16.8155\n",
      "Epoch 252/800\n",
      "8/8 [==============================] - 6s 793ms/step - loss: 16.1868 - val_loss: 16.9508\n",
      "Epoch 253/800\n",
      "8/8 [==============================] - 6s 806ms/step - loss: 15.9475 - val_loss: 16.3033\n",
      "Epoch 254/800\n",
      "8/8 [==============================] - 6s 797ms/step - loss: 16.3076 - val_loss: 15.4340\n",
      "Epoch 255/800\n",
      "8/8 [==============================] - 6s 795ms/step - loss: 16.3686 - val_loss: 16.2663\n",
      "Epoch 256/800\n",
      "8/8 [==============================] - 6s 795ms/step - loss: 16.0058 - val_loss: 16.0820\n",
      "Epoch 257/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 15.6747 - val_loss: 15.9665\n",
      "Epoch 258/800\n",
      "8/8 [==============================] - 6s 787ms/step - loss: 15.6259 - val_loss: 14.4796\n",
      "Epoch 259/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 15.8647 - val_loss: 15.2494\n",
      "Epoch 260/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 14.9369 - val_loss: 16.4321\n",
      "Epoch 261/800\n",
      "8/8 [==============================] - 6s 808ms/step - loss: 15.5902 - val_loss: 15.2749\n",
      "Epoch 262/800\n",
      "8/8 [==============================] - 6s 797ms/step - loss: 15.3392 - val_loss: 15.5392\n",
      "Epoch 263/800\n",
      "8/8 [==============================] - 6s 798ms/step - loss: 14.9346 - val_loss: 15.2229\n",
      "Epoch 280/800\n",
      "8/8 [==============================] - 6s 781ms/step - loss: 14.9556 - val_loss: 14.7372\n",
      "Epoch 281/800\n",
      "8/8 [==============================] - 6s 789ms/step - loss: 14.4945 - val_loss: 14.7897\n",
      "Epoch 282/800\n",
      "8/8 [==============================] - 6s 793ms/step - loss: 14.7796 - val_loss: 14.5418\n",
      "Epoch 283/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 15.5670 - val_loss: 13.9967\n",
      "Epoch 284/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 14.5511 - val_loss: 15.5982\n",
      "Epoch 285/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 15.0383 - val_loss: 13.8016\n",
      "Epoch 286/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 14.7032 - val_loss: 14.0365\n",
      "Epoch 287/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 14.7119 - val_loss: 13.5764\n",
      "Epoch 288/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 15.2185 - val_loss: 15.0849\n",
      "Epoch 289/800\n",
      "8/8 [==============================] - 6s 798ms/step - loss: 14.5077 - val_loss: 13.8491\n",
      "Epoch 290/800\n",
      "8/8 [==============================] - 6s 785ms/step - loss: 14.3276 - val_loss: 15.0470\n",
      "Epoch 291/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 14.8108 - val_loss: 14.1522\n",
      "Epoch 292/800\n",
      "8/8 [==============================] - 6s 809ms/step - loss: 14.5343 - val_loss: 13.7423\n",
      "Epoch 293/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 14.7215 - val_loss: 13.9171\n",
      "Epoch 294/800\n",
      "8/8 [==============================] - 6s 806ms/step - loss: 14.6698 - val_loss: 14.6371\n",
      "Epoch 295/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 14.3425 - val_loss: 13.7965\n",
      "Epoch 296/800\n",
      "8/8 [==============================] - 6s 806ms/step - loss: 13.6409 - val_loss: 13.6103\n",
      "Epoch 297/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 14.6702 - val_loss: 14.8890\n",
      "Epoch 298/800\n",
      "8/8 [==============================] - 6s 790ms/step - loss: 14.8012 - val_loss: 14.5356\n",
      "Epoch 299/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 14.4977 - val_loss: 15.5470\n",
      "Epoch 300/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 14.1895 - val_loss: 15.6270\n",
      "Epoch 301/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 14.3233 - val_loss: 13.5052\n",
      "Epoch 302/800\n",
      "8/8 [==============================] - 6s 789ms/step - loss: 14.8249 - val_loss: 14.8375\n",
      "Epoch 303/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 14.3592 - val_loss: 14.4544\n",
      "Epoch 304/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 13.9830 - val_loss: 13.5216\n",
      "Epoch 305/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 14.5385 - val_loss: 14.3672\n",
      "Epoch 306/800\n",
      "8/8 [==============================] - 6s 786ms/step - loss: 14.2778 - val_loss: 14.5708\n",
      "Epoch 307/800\n",
      "8/8 [==============================] - 6s 807ms/step - loss: 14.1512 - val_loss: 15.1821\n",
      "Epoch 308/800\n",
      "8/8 [==============================] - 6s 786ms/step - loss: 14.0563 - val_loss: 13.3479\n",
      "Epoch 309/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 6s 801ms/step - loss: 14.7285 - val_loss: 13.7660\n",
      "Epoch 310/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 14.3575 - val_loss: 13.7663\n",
      "Epoch 311/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 14.3870 - val_loss: 13.9409\n",
      "Epoch 312/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 13.7566 - val_loss: 13.7336\n",
      "Epoch 313/800\n",
      "8/8 [==============================] - 6s 798ms/step - loss: 13.9253 - val_loss: 13.8587\n",
      "Epoch 314/800\n",
      "8/8 [==============================] - 6s 798ms/step - loss: 13.9736 - val_loss: 13.7049\n",
      "Epoch 315/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 13.9854 - val_loss: 14.3967\n",
      "Epoch 316/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 14.2657 - val_loss: 14.2136\n",
      "Epoch 317/800\n",
      "8/8 [==============================] - 6s 787ms/step - loss: 13.6328 - val_loss: 13.5205\n",
      "Epoch 318/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 14.0465 - val_loss: 12.9199\n",
      "Epoch 319/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 13.9455 - val_loss: 13.6073\n",
      "Epoch 320/800\n",
      "8/8 [==============================] - 6s 787ms/step - loss: 14.0846 - val_loss: 13.9665\n",
      "Epoch 321/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 14.3080 - val_loss: 14.0803\n",
      "Epoch 322/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 13.8047 - val_loss: 12.6487\n",
      "Epoch 323/800\n",
      "8/8 [==============================] - 6s 797ms/step - loss: 14.0490 - val_loss: 13.6260\n",
      "Epoch 324/800\n",
      "8/8 [==============================] - 6s 785ms/step - loss: 14.4768 - val_loss: 13.6824\n",
      "Epoch 325/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 14.0128 - val_loss: 13.9684\n",
      "Epoch 326/800\n",
      "8/8 [==============================] - 6s 795ms/step - loss: 13.8685 - val_loss: 13.7674\n",
      "Epoch 327/800\n",
      "8/8 [==============================] - 6s 787ms/step - loss: 13.9619 - val_loss: 14.5992\n",
      "Epoch 328/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 13.4892 - val_loss: 14.3924\n",
      "Epoch 329/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 13.9314 - val_loss: 14.2209\n",
      "Epoch 330/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 13.5948 - val_loss: 13.3813\n",
      "Epoch 331/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 13.4968 - val_loss: 12.8373\n",
      "Epoch 332/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 13.3595 - val_loss: 14.1092\n",
      "Epoch 333/800\n",
      "8/8 [==============================] - 6s 808ms/step - loss: 13.3512 - val_loss: 13.5199\n",
      "Epoch 334/800\n",
      "8/8 [==============================] - 6s 806ms/step - loss: 13.9418 - val_loss: 13.5655\n",
      "Epoch 335/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 13.6536 - val_loss: 12.9128\n",
      "Epoch 336/800\n",
      "8/8 [==============================] - 6s 806ms/step - loss: 13.6491 - val_loss: 13.1928\n",
      "Epoch 337/800\n",
      "8/8 [==============================] - 6s 796ms/step - loss: 13.3139 - val_loss: 14.4908\n",
      "Epoch 338/800\n",
      "8/8 [==============================] - 6s 790ms/step - loss: 13.8127 - val_loss: 13.2318\n",
      "Epoch 339/800\n",
      "8/8 [==============================] - 6s 807ms/step - loss: 13.2329 - val_loss: 11.5796\n",
      "Epoch 340/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 13.3838 - val_loss: 13.6548\n",
      "Epoch 341/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 13.0244 - val_loss: 13.5227\n",
      "Epoch 342/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 13.0918 - val_loss: 12.0767\n",
      "Epoch 343/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 13.0175 - val_loss: 14.1245\n",
      "Epoch 344/800\n",
      "8/8 [==============================] - 6s 812ms/step - loss: 13.0133 - val_loss: 12.9128\n",
      "Epoch 345/800\n",
      "8/8 [==============================] - 6s 809ms/step - loss: 13.5532 - val_loss: 12.4337\n",
      "Epoch 346/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 13.2806 - val_loss: 13.0201\n",
      "Epoch 347/800\n",
      "8/8 [==============================] - 6s 790ms/step - loss: 13.4273 - val_loss: 13.1129\n",
      "Epoch 348/800\n",
      "8/8 [==============================] - 6s 792ms/step - loss: 13.2622 - val_loss: 14.2402\n",
      "Epoch 349/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 13.2731 - val_loss: 14.7629\n",
      "Epoch 350/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 13.2037 - val_loss: 13.4134\n",
      "Epoch 351/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 12.7500 - val_loss: 13.8081\n",
      "Epoch 352/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 13.2590 - val_loss: 11.6793\n",
      "Epoch 353/800\n",
      "8/8 [==============================] - 6s 786ms/step - loss: 13.5285 - val_loss: 13.8776\n",
      "Epoch 354/800\n",
      "8/8 [==============================] - 6s 808ms/step - loss: 13.4815 - val_loss: 13.3638\n",
      "Epoch 355/800\n",
      "8/8 [==============================] - 6s 807ms/step - loss: 13.1547 - val_loss: 12.0711\n",
      "Epoch 356/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 13.1801 - val_loss: 11.9265\n",
      "Epoch 357/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 13.0383 - val_loss: 12.5254\n",
      "Epoch 358/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 12.5943 - val_loss: 12.7915\n",
      "Epoch 359/800\n",
      "8/8 [==============================] - 6s 798ms/step - loss: 13.4710 - val_loss: 13.3561\n",
      "Epoch 360/800\n",
      "8/8 [==============================] - 7s 813ms/step - loss: 13.0888 - val_loss: 12.2233\n",
      "Epoch 361/800\n",
      "8/8 [==============================] - 6s 797ms/step - loss: 12.6048 - val_loss: 12.2491\n",
      "Epoch 362/800\n",
      "8/8 [==============================] - 6s 795ms/step - loss: 12.7749 - val_loss: 11.6452\n",
      "Epoch 363/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 12.7600 - val_loss: 12.7822\n",
      "Epoch 364/800\n",
      "8/8 [==============================] - 6s 786ms/step - loss: 12.8602 - val_loss: 11.6223\n",
      "Epoch 365/800\n",
      "8/8 [==============================] - 6s 796ms/step - loss: 12.7686 - val_loss: 12.6353\n",
      "Epoch 366/800\n",
      "8/8 [==============================] - 6s 789ms/step - loss: 12.4924 - val_loss: 11.7265\n",
      "Epoch 367/800\n",
      "8/8 [==============================] - 6s 798ms/step - loss: 12.6810 - val_loss: 12.0781\n",
      "Epoch 368/800\n",
      "8/8 [==============================] - 6s 791ms/step - loss: 12.4058 - val_loss: 12.4652\n",
      "Epoch 369/800\n",
      "8/8 [==============================] - 6s 784ms/step - loss: 12.4227 - val_loss: 12.7492\n",
      "Epoch 370/800\n",
      "8/8 [==============================] - 6s 795ms/step - loss: 12.7766 - val_loss: 12.2842\n",
      "Epoch 371/800\n",
      "8/8 [==============================] - 6s 795ms/step - loss: 12.5942 - val_loss: 12.7833\n",
      "Epoch 372/800\n",
      "8/8 [==============================] - 6s 785ms/step - loss: 12.9844 - val_loss: 12.2361\n",
      "Epoch 373/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 12.5964 - val_loss: 13.2632\n",
      "Epoch 374/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 12.9361 - val_loss: 12.3973\n",
      "Epoch 375/800\n",
      "8/8 [==============================] - 6s 787ms/step - loss: 12.4230 - val_loss: 12.7554\n",
      "Epoch 376/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 13.0425 - val_loss: 12.9110\n",
      "Epoch 377/800\n",
      "8/8 [==============================] - 6s 798ms/step - loss: 12.2880 - val_loss: 12.2985\n",
      "Epoch 378/800\n",
      "8/8 [==============================] - 6s 795ms/step - loss: 12.6427 - val_loss: 12.6857\n",
      "Epoch 379/800\n",
      "8/8 [==============================] - 6s 789ms/step - loss: 12.1781 - val_loss: 13.0626\n",
      "Epoch 380/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 12.1139 - val_loss: 11.9311\n",
      "Epoch 381/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 12.1880 - val_loss: 14.8046\n",
      "Epoch 382/800\n",
      "8/8 [==============================] - 6s 808ms/step - loss: 12.5577 - val_loss: 11.5187\n",
      "Epoch 383/800\n",
      "8/8 [==============================] - 6s 783ms/step - loss: 13.3182 - val_loss: 13.7299\n",
      "Epoch 384/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 12.6582 - val_loss: 11.5913\n",
      "Epoch 385/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 12.4248 - val_loss: 12.2455\n",
      "Epoch 386/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 12.4267 - val_loss: 11.2223\n",
      "Epoch 387/800\n",
      "8/8 [==============================] - 6s 783ms/step - loss: 12.6119 - val_loss: 12.9218\n",
      "Epoch 388/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 6s 801ms/step - loss: 12.7363 - val_loss: 14.2840\n",
      "Epoch 389/800\n",
      "8/8 [==============================] - 6s 798ms/step - loss: 12.0309 - val_loss: 12.6023\n",
      "Epoch 390/800\n",
      "8/8 [==============================] - 6s 796ms/step - loss: 12.5445 - val_loss: 12.6257\n",
      "Epoch 391/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 12.4982 - val_loss: 11.6064\n",
      "Epoch 392/800\n",
      "8/8 [==============================] - 6s 806ms/step - loss: 12.5236 - val_loss: 13.7738\n",
      "Epoch 393/800\n",
      "8/8 [==============================] - 6s 789ms/step - loss: 12.5515 - val_loss: 13.0865\n",
      "Epoch 394/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 12.4418 - val_loss: 12.0044\n",
      "Epoch 395/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 11.9785 - val_loss: 11.1801\n",
      "Epoch 396/800\n",
      "8/8 [==============================] - 6s 796ms/step - loss: 11.9954 - val_loss: 12.5742\n",
      "Epoch 397/800\n",
      "8/8 [==============================] - 6s 782ms/step - loss: 12.1833 - val_loss: 12.2277\n",
      "Epoch 398/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 12.0544 - val_loss: 11.5143\n",
      "Epoch 399/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 12.1606 - val_loss: 11.8555\n",
      "Epoch 400/800\n",
      "8/8 [==============================] - 6s 807ms/step - loss: 11.9831 - val_loss: 12.5377\n",
      "Epoch 401/800\n",
      "8/8 [==============================] - 6s 807ms/step - loss: 11.9958 - val_loss: 12.0115\n",
      "Epoch 402/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 12.2389 - val_loss: 11.2067\n",
      "Epoch 403/800\n",
      "8/8 [==============================] - 6s 786ms/step - loss: 12.0779 - val_loss: 11.2387\n",
      "Epoch 404/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 12.3599 - val_loss: 11.2810\n",
      "Epoch 405/800\n",
      "8/8 [==============================] - 6s 806ms/step - loss: 11.9565 - val_loss: 11.8830\n",
      "Epoch 406/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 12.3059 - val_loss: 11.9835\n",
      "Epoch 407/800\n",
      "8/8 [==============================] - 6s 806ms/step - loss: 11.9880 - val_loss: 12.6828\n",
      "Epoch 408/800\n",
      "8/8 [==============================] - 6s 795ms/step - loss: 12.3732 - val_loss: 10.9043\n",
      "Epoch 409/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 12.2160 - val_loss: 11.3094\n",
      "Epoch 410/800\n",
      "8/8 [==============================] - 6s 797ms/step - loss: 11.8431 - val_loss: 11.0341\n",
      "Epoch 411/800\n",
      "8/8 [==============================] - 6s 791ms/step - loss: 11.9969 - val_loss: 11.7268\n",
      "Epoch 412/800\n",
      "8/8 [==============================] - 6s 798ms/step - loss: 11.7068 - val_loss: 10.6570\n",
      "Epoch 413/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 11.7844 - val_loss: 12.2555\n",
      "Epoch 414/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 11.7627 - val_loss: 11.7188\n",
      "Epoch 415/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 12.0880 - val_loss: 11.3683\n",
      "Epoch 416/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 11.7496 - val_loss: 11.0952\n",
      "Epoch 417/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 12.3307 - val_loss: 12.2189\n",
      "Epoch 418/800\n",
      "8/8 [==============================] - 6s 797ms/step - loss: 12.2610 - val_loss: 11.5670\n",
      "Epoch 419/800\n",
      "8/8 [==============================] - 6s 807ms/step - loss: 11.5815 - val_loss: 12.2375\n",
      "Epoch 420/800\n",
      "8/8 [==============================] - 6s 811ms/step - loss: 11.9043 - val_loss: 12.6616\n",
      "Epoch 421/800\n",
      "8/8 [==============================] - 6s 796ms/step - loss: 11.5221 - val_loss: 11.7494\n",
      "Epoch 422/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 11.6234 - val_loss: 12.1255\n",
      "Epoch 423/800\n",
      "8/8 [==============================] - 6s 784ms/step - loss: 11.4600 - val_loss: 12.0887\n",
      "Epoch 424/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 11.8425 - val_loss: 11.1241\n",
      "Epoch 425/800\n",
      "8/8 [==============================] - 6s 796ms/step - loss: 11.6798 - val_loss: 12.1504\n",
      "Epoch 426/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 12.1995 - val_loss: 10.8879\n",
      "Epoch 427/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 11.7472 - val_loss: 10.9959\n",
      "Epoch 428/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 11.5485 - val_loss: 10.9994\n",
      "Epoch 429/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 12.3072 - val_loss: 10.4667\n",
      "Epoch 430/800\n",
      "8/8 [==============================] - 6s 798ms/step - loss: 11.5450 - val_loss: 10.7859\n",
      "Epoch 431/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 11.3244 - val_loss: 12.1501\n",
      "Epoch 432/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 10.9872 - val_loss: 10.8067\n",
      "Epoch 433/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 11.3650 - val_loss: 10.6599\n",
      "Epoch 434/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 11.5190 - val_loss: 11.9118\n",
      "Epoch 435/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 11.6762 - val_loss: 11.6683\n",
      "Epoch 436/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 11.6000 - val_loss: 11.9781\n",
      "Epoch 437/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 11.8614 - val_loss: 11.9854\n",
      "Epoch 438/800\n",
      "8/8 [==============================] - 6s 793ms/step - loss: 11.2219 - val_loss: 11.2495\n",
      "Epoch 439/800\n",
      "8/8 [==============================] - 6s 795ms/step - loss: 11.4444 - val_loss: 13.2632\n",
      "Epoch 440/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 11.7739 - val_loss: 11.0105\n",
      "Epoch 441/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 11.5150 - val_loss: 11.5737\n",
      "Epoch 442/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 11.6226 - val_loss: 10.9751\n",
      "Epoch 443/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 11.9134 - val_loss: 10.2964\n",
      "Epoch 444/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 11.3105 - val_loss: 11.6113\n",
      "Epoch 445/800\n",
      "8/8 [==============================] - 6s 785ms/step - loss: 11.4705 - val_loss: 10.2148\n",
      "Epoch 446/800\n",
      "8/8 [==============================] - 6s 808ms/step - loss: 11.4016 - val_loss: 10.6670\n",
      "Epoch 447/800\n",
      "8/8 [==============================] - 6s 798ms/step - loss: 11.4327 - val_loss: 12.9514\n",
      "Epoch 448/800\n",
      "8/8 [==============================] - 6s 795ms/step - loss: 12.0102 - val_loss: 11.5916\n",
      "Epoch 449/800\n",
      "8/8 [==============================] - 6s 791ms/step - loss: 11.1852 - val_loss: 10.7926\n",
      "Epoch 450/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 11.6943 - val_loss: 10.2978\n",
      "Epoch 451/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 11.2181 - val_loss: 11.2689\n",
      "Epoch 452/800\n",
      "8/8 [==============================] - 6s 794ms/step - loss: 11.2050 - val_loss: 10.1747\n",
      "Epoch 453/800\n",
      "8/8 [==============================] - 6s 806ms/step - loss: 11.2561 - val_loss: 9.7980\n",
      "Epoch 454/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 11.1643 - val_loss: 11.1194\n",
      "Epoch 455/800\n",
      "8/8 [==============================] - 6s 808ms/step - loss: 11.1935 - val_loss: 11.5807\n",
      "Epoch 456/800\n",
      "8/8 [==============================] - 6s 786ms/step - loss: 11.3796 - val_loss: 10.2070\n",
      "Epoch 457/800\n",
      "8/8 [==============================] - 6s 789ms/step - loss: 10.9252 - val_loss: 11.5175\n",
      "Epoch 458/800\n",
      "8/8 [==============================] - 6s 798ms/step - loss: 10.8862 - val_loss: 11.8582\n",
      "Epoch 459/800\n",
      "8/8 [==============================] - 6s 797ms/step - loss: 11.4609 - val_loss: 11.1759\n",
      "Epoch 460/800\n",
      "8/8 [==============================] - 6s 798ms/step - loss: 11.4067 - val_loss: 11.0428\n",
      "Epoch 461/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 11.3306 - val_loss: 11.0054\n",
      "Epoch 462/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 10.9679 - val_loss: 10.7839\n",
      "Epoch 463/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 11.1905 - val_loss: 10.7174\n",
      "Epoch 464/800\n",
      "8/8 [==============================] - 6s 786ms/step - loss: 11.1777 - val_loss: 10.3426\n",
      "Epoch 465/800\n",
      "8/8 [==============================] - 6s 791ms/step - loss: 11.1881 - val_loss: 11.0745\n",
      "Epoch 466/800\n",
      "8/8 [==============================] - 6s 790ms/step - loss: 11.4959 - val_loss: 10.2516\n",
      "Epoch 467/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 6s 801ms/step - loss: 10.8976 - val_loss: 10.7175\n",
      "Epoch 468/800\n",
      "8/8 [==============================] - 6s 792ms/step - loss: 10.9027 - val_loss: 11.2537\n",
      "Epoch 469/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 11.6446 - val_loss: 10.4706\n",
      "Epoch 470/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 10.7430 - val_loss: 10.4168\n",
      "Epoch 471/800\n",
      "8/8 [==============================] - 6s 792ms/step - loss: 11.0035 - val_loss: 10.1760\n",
      "Epoch 472/800\n",
      "8/8 [==============================] - 6s 808ms/step - loss: 10.8114 - val_loss: 9.6426\n",
      "Epoch 473/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 10.5551 - val_loss: 10.2310\n",
      "Epoch 474/800\n",
      "8/8 [==============================] - 6s 785ms/step - loss: 10.6011 - val_loss: 11.2308\n",
      "Epoch 475/800\n",
      "8/8 [==============================] - 6s 806ms/step - loss: 10.7175 - val_loss: 12.7859\n",
      "Epoch 476/800\n",
      "8/8 [==============================] - 6s 785ms/step - loss: 10.8656 - val_loss: 11.0638\n",
      "Epoch 477/800\n",
      "8/8 [==============================] - 6s 807ms/step - loss: 10.6994 - val_loss: 10.0160\n",
      "Epoch 478/800\n",
      "8/8 [==============================] - 6s 808ms/step - loss: 10.6147 - val_loss: 10.6496\n",
      "Epoch 479/800\n",
      "8/8 [==============================] - 6s 808ms/step - loss: 10.1696 - val_loss: 10.8635\n",
      "Epoch 480/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 10.8130 - val_loss: 10.5918\n",
      "Epoch 481/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 10.8268 - val_loss: 10.2036\n",
      "Epoch 482/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 10.4422 - val_loss: 10.3348\n",
      "Epoch 483/800\n",
      "8/8 [==============================] - 6s 807ms/step - loss: 11.3569 - val_loss: 11.2980\n",
      "Epoch 484/800\n",
      "8/8 [==============================] - 6s 796ms/step - loss: 10.8029 - val_loss: 10.8395\n",
      "Epoch 485/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 10.7302 - val_loss: 10.4287\n",
      "Epoch 486/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 10.6360 - val_loss: 11.4271\n",
      "Epoch 487/800\n",
      "8/8 [==============================] - 6s 785ms/step - loss: 11.6022 - val_loss: 9.4695\n",
      "Epoch 488/800\n",
      "8/8 [==============================] - 6s 798ms/step - loss: 11.0628 - val_loss: 10.4881\n",
      "Epoch 489/800\n",
      "8/8 [==============================] - 6s 795ms/step - loss: 10.8339 - val_loss: 11.3239\n",
      "Epoch 490/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 10.4720 - val_loss: 10.8387\n",
      "Epoch 491/800\n",
      "8/8 [==============================] - 6s 789ms/step - loss: 10.4931 - val_loss: 9.8612\n",
      "Epoch 492/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 10.6589 - val_loss: 12.5420\n",
      "Epoch 493/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 10.8496 - val_loss: 10.1676\n",
      "Epoch 494/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 10.3497 - val_loss: 10.8372\n",
      "Epoch 495/800\n",
      "8/8 [==============================] - 6s 781ms/step - loss: 10.4021 - val_loss: 11.2581\n",
      "Epoch 496/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 10.0907 - val_loss: 10.1642\n",
      "Epoch 497/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 10.4270 - val_loss: 10.1335\n",
      "Epoch 498/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 10.8448 - val_loss: 10.1511\n",
      "Epoch 499/800\n",
      "8/8 [==============================] - 6s 798ms/step - loss: 10.7013 - val_loss: 9.9002\n",
      "Epoch 500/800\n",
      "8/8 [==============================] - 6s 794ms/step - loss: 11.3214 - val_loss: 9.8409\n",
      "Epoch 501/800\n",
      "8/8 [==============================] - 6s 797ms/step - loss: 10.1624 - val_loss: 10.0471\n",
      "Epoch 502/800\n",
      "8/8 [==============================] - 6s 796ms/step - loss: 10.8016 - val_loss: 10.3501\n",
      "Epoch 503/800\n",
      "8/8 [==============================] - 6s 787ms/step - loss: 10.6151 - val_loss: 10.6065\n",
      "Epoch 504/800\n",
      "8/8 [==============================] - 6s 806ms/step - loss: 10.2612 - val_loss: 10.8866\n",
      "Epoch 505/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 10.5702 - val_loss: 10.3485\n",
      "Epoch 506/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 10.4567 - val_loss: 11.4686\n",
      "Epoch 507/800\n",
      "8/8 [==============================] - 6s 807ms/step - loss: 10.5883 - val_loss: 10.1660\n",
      "Epoch 508/800\n",
      "8/8 [==============================] - 6s 793ms/step - loss: 10.2155 - val_loss: 9.9772\n",
      "Epoch 509/800\n",
      "8/8 [==============================] - 6s 787ms/step - loss: 9.9863 - val_loss: 11.5098\n",
      "Epoch 510/800\n",
      "8/8 [==============================] - 6s 797ms/step - loss: 9.9802 - val_loss: 9.8659\n",
      "Epoch 511/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 10.0699 - val_loss: 10.0973\n",
      "Epoch 512/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 10.1458 - val_loss: 10.4781\n",
      "Epoch 513/800\n",
      "8/8 [==============================] - 6s 807ms/step - loss: 9.9254 - val_loss: 10.1839\n",
      "Epoch 514/800\n",
      "8/8 [==============================] - 6s 796ms/step - loss: 10.0041 - val_loss: 9.8149\n",
      "Epoch 515/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 10.2159 - val_loss: 9.6557\n",
      "Epoch 516/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 10.2763 - val_loss: 10.7178\n",
      "Epoch 517/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 10.1826 - val_loss: 9.2728\n",
      "Epoch 518/800\n",
      "8/8 [==============================] - 6s 786ms/step - loss: 10.5488 - val_loss: 11.4175\n",
      "Epoch 519/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 10.8728 - val_loss: 10.5159\n",
      "Epoch 520/800\n",
      "8/8 [==============================] - 6s 794ms/step - loss: 10.3769 - val_loss: 9.3299\n",
      "Epoch 521/800\n",
      "8/8 [==============================] - 6s 786ms/step - loss: 10.3188 - val_loss: 10.5372\n",
      "Epoch 522/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 10.0574 - val_loss: 10.0119\n",
      "Epoch 523/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 10.0826 - val_loss: 10.4506\n",
      "Epoch 524/800\n",
      "8/8 [==============================] - 6s 807ms/step - loss: 10.2210 - val_loss: 10.2633\n",
      "Epoch 525/800\n",
      "8/8 [==============================] - 6s 812ms/step - loss: 10.2441 - val_loss: 10.7042\n",
      "Epoch 526/800\n",
      "8/8 [==============================] - 6s 798ms/step - loss: 10.5235 - val_loss: 10.4853\n",
      "Epoch 527/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 10.2651 - val_loss: 9.6878\n",
      "Epoch 528/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 10.0509 - val_loss: 11.2985\n",
      "Epoch 529/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 10.4733 - val_loss: 10.1913\n",
      "Epoch 530/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 9.8591 - val_loss: 9.0308\n",
      "Epoch 531/800\n",
      "8/8 [==============================] - 6s 809ms/step - loss: 9.8652 - val_loss: 9.2883\n",
      "Epoch 532/800\n",
      "8/8 [==============================] - 6s 796ms/step - loss: 9.7640 - val_loss: 10.2123\n",
      "Epoch 533/800\n",
      "8/8 [==============================] - 6s 789ms/step - loss: 10.1698 - val_loss: 10.4971\n",
      "Epoch 534/800\n",
      "8/8 [==============================] - 6s 789ms/step - loss: 10.0287 - val_loss: 8.6868\n",
      "Epoch 535/800\n",
      "8/8 [==============================] - 6s 807ms/step - loss: 10.1303 - val_loss: 10.6715\n",
      "Epoch 536/800\n",
      "8/8 [==============================] - 6s 794ms/step - loss: 9.9097 - val_loss: 10.5494\n",
      "Epoch 537/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 10.0711 - val_loss: 9.1489\n",
      "Epoch 538/800\n",
      "8/8 [==============================] - 6s 807ms/step - loss: 9.8265 - val_loss: 8.9917\n",
      "Epoch 539/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 9.8064 - val_loss: 9.7299\n",
      "Epoch 540/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 9.7959 - val_loss: 10.8679\n",
      "Epoch 541/800\n",
      "8/8 [==============================] - 6s 796ms/step - loss: 9.9979 - val_loss: 8.9177\n",
      "Epoch 542/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 9.8255 - val_loss: 9.6925\n",
      "Epoch 543/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 9.9346 - val_loss: 9.0637\n",
      "Epoch 544/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 9.9949 - val_loss: 9.6911\n",
      "Epoch 545/800\n",
      "8/8 [==============================] - 6s 798ms/step - loss: 10.0097 - val_loss: 9.0682\n",
      "Epoch 546/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 10.3889 - val_loss: 9.5988\n",
      "Epoch 547/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 6s 791ms/step - loss: 9.7003 - val_loss: 10.8581\n",
      "Epoch 548/800\n",
      "8/8 [==============================] - 6s 792ms/step - loss: 9.5166 - val_loss: 10.3522\n",
      "Epoch 549/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 9.8776 - val_loss: 10.0238\n",
      "Epoch 550/800\n",
      "8/8 [==============================] - 6s 784ms/step - loss: 9.3165 - val_loss: 10.3968\n",
      "Epoch 551/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 9.8549 - val_loss: 10.1298\n",
      "Epoch 552/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 9.7049 - val_loss: 8.7784\n",
      "Epoch 553/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 9.9358 - val_loss: 9.2021\n",
      "Epoch 554/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 9.6982 - val_loss: 11.0537\n",
      "Epoch 555/800\n",
      "8/8 [==============================] - 6s 787ms/step - loss: 9.7786 - val_loss: 10.2744\n",
      "Epoch 556/800\n",
      "8/8 [==============================] - 6s 795ms/step - loss: 9.6807 - val_loss: 9.6383\n",
      "Epoch 557/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 9.4064 - val_loss: 9.9163\n",
      "Epoch 558/800\n",
      "8/8 [==============================] - 6s 806ms/step - loss: 9.6783 - val_loss: 10.9370\n",
      "Epoch 559/800\n",
      "8/8 [==============================] - 6s 808ms/step - loss: 9.4119 - val_loss: 10.4768\n",
      "Epoch 560/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 9.9410 - val_loss: 10.3019\n",
      "Epoch 561/800\n",
      "8/8 [==============================] - 6s 795ms/step - loss: 9.9610 - val_loss: 13.7601\n",
      "Epoch 562/800\n",
      "8/8 [==============================] - 6s 802ms/step - loss: 10.0179 - val_loss: 9.2922\n",
      "Epoch 563/800\n",
      "8/8 [==============================] - 6s 808ms/step - loss: 9.7725 - val_loss: 9.1120\n",
      "Epoch 564/800\n",
      "8/8 [==============================] - 6s 794ms/step - loss: 9.6597 - val_loss: 8.6348\n",
      "Epoch 565/800\n",
      "8/8 [==============================] - 6s 807ms/step - loss: 9.3494 - val_loss: 9.0833\n",
      "Epoch 566/800\n",
      "8/8 [==============================] - 6s 794ms/step - loss: 9.9794 - val_loss: 9.9304\n",
      "Epoch 567/800\n",
      "8/8 [==============================] - 6s 810ms/step - loss: 9.1641 - val_loss: 10.7435\n",
      "Epoch 568/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 9.5888 - val_loss: 8.9560\n",
      "Epoch 569/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 9.4146 - val_loss: 9.7976\n",
      "Epoch 570/800\n",
      "8/8 [==============================] - 6s 807ms/step - loss: 9.5356 - val_loss: 9.2394\n",
      "Epoch 571/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 10.0414 - val_loss: 9.6760\n",
      "Epoch 572/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 9.6653 - val_loss: 9.2933\n",
      "Epoch 573/800\n",
      "8/8 [==============================] - 6s 798ms/step - loss: 9.6567 - val_loss: 9.0856\n",
      "Epoch 574/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 9.2893 - val_loss: 11.0114\n",
      "Epoch 575/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 9.6793 - val_loss: 8.8482\n",
      "Epoch 576/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 9.1879 - val_loss: 9.2441\n",
      "Epoch 577/800\n",
      "8/8 [==============================] - 6s 806ms/step - loss: 9.6318 - val_loss: 11.0373\n",
      "Epoch 578/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 9.4021 - val_loss: 9.3122\n",
      "Epoch 579/800\n",
      "8/8 [==============================] - 6s 796ms/step - loss: 9.5191 - val_loss: 8.7027\n",
      "Epoch 580/800\n",
      "8/8 [==============================] - 6s 783ms/step - loss: 10.0111 - val_loss: 10.2227\n",
      "Epoch 581/800\n",
      "8/8 [==============================] - 6s 782ms/step - loss: 9.5530 - val_loss: 9.3266\n",
      "Epoch 582/800\n",
      "8/8 [==============================] - 6s 785ms/step - loss: 9.3700 - val_loss: 11.9166\n",
      "Epoch 583/800\n",
      "8/8 [==============================] - 6s 807ms/step - loss: 9.4898 - val_loss: 10.7148\n",
      "Epoch 584/800\n",
      "8/8 [==============================] - 6s 785ms/step - loss: 9.6500 - val_loss: 9.8190\n",
      "Epoch 585/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 9.6204 - val_loss: 10.0693\n",
      "Epoch 586/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 10.0229 - val_loss: 9.8331\n",
      "Epoch 587/800\n",
      "8/8 [==============================] - 6s 782ms/step - loss: 9.7248 - val_loss: 10.1101\n",
      "Epoch 588/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 9.4919 - val_loss: 9.8474\n",
      "Epoch 589/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 9.6527 - val_loss: 10.8998\n",
      "Epoch 590/800\n",
      "8/8 [==============================] - 6s 786ms/step - loss: 9.6074 - val_loss: 8.5397\n",
      "Epoch 591/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 9.3510 - val_loss: 8.6608\n",
      "Epoch 592/800\n",
      "8/8 [==============================] - 6s 790ms/step - loss: 9.6713 - val_loss: 8.6478\n",
      "Epoch 593/800\n",
      "8/8 [==============================] - 6s 798ms/step - loss: 9.4041 - val_loss: 8.7074\n",
      "Epoch 594/800\n",
      "8/8 [==============================] - 6s 797ms/step - loss: 9.7588 - val_loss: 9.1320\n",
      "Epoch 595/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 9.0946 - val_loss: 8.6487\n",
      "Epoch 596/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 9.0891 - val_loss: 8.7654\n",
      "Epoch 597/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 9.0509 - val_loss: 11.4981\n",
      "Epoch 598/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 9.3256 - val_loss: 8.4252\n",
      "Epoch 599/800\n",
      "8/8 [==============================] - 6s 806ms/step - loss: 8.7369 - val_loss: 8.4460\n",
      "Epoch 600/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 9.5411 - val_loss: 9.5493\n",
      "Epoch 601/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 9.3577 - val_loss: 10.1878\n",
      "Epoch 602/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 9.5568 - val_loss: 9.8924\n",
      "Epoch 603/800\n",
      "8/8 [==============================] - 6s 806ms/step - loss: 9.2517 - val_loss: 8.2108\n",
      "Epoch 604/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 9.4699 - val_loss: 9.8866\n",
      "Epoch 605/800\n",
      "8/8 [==============================] - 6s 785ms/step - loss: 9.2598 - val_loss: 14.1770\n",
      "Epoch 606/800\n",
      "8/8 [==============================] - 6s 809ms/step - loss: 9.2166 - val_loss: 10.4996\n",
      "Epoch 607/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 9.1106 - val_loss: 9.6918\n",
      "Epoch 608/800\n",
      "8/8 [==============================] - 6s 806ms/step - loss: 9.1977 - val_loss: 9.4853\n",
      "Epoch 609/800\n",
      "8/8 [==============================] - 6s 808ms/step - loss: 9.1354 - val_loss: 8.9115\n",
      "Epoch 610/800\n",
      "8/8 [==============================] - 6s 806ms/step - loss: 9.3036 - val_loss: 10.2283\n",
      "Epoch 611/800\n",
      "8/8 [==============================] - 6s 807ms/step - loss: 9.4349 - val_loss: 9.2951\n",
      "Epoch 612/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 9.8407 - val_loss: 8.8393\n",
      "Epoch 613/800\n",
      "8/8 [==============================] - 6s 798ms/step - loss: 9.2069 - val_loss: 8.4742\n",
      "Epoch 614/800\n",
      "8/8 [==============================] - 6s 796ms/step - loss: 8.8784 - val_loss: 9.6851\n",
      "Epoch 615/800\n",
      "8/8 [==============================] - 6s 800ms/step - loss: 9.9389 - val_loss: 10.0733\n",
      "Epoch 616/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 9.1825 - val_loss: 8.8067\n",
      "Epoch 617/800\n",
      "8/8 [==============================] - 6s 808ms/step - loss: 9.0888 - val_loss: 8.5054\n",
      "Epoch 618/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 9.2096 - val_loss: 11.0106\n",
      "Epoch 619/800\n",
      "8/8 [==============================] - 6s 798ms/step - loss: 9.2806 - val_loss: 9.0658\n",
      "Epoch 620/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 9.1118 - val_loss: 8.6820\n",
      "Epoch 621/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 9.2604 - val_loss: 7.7619\n",
      "Epoch 622/800\n",
      "8/8 [==============================] - 6s 792ms/step - loss: 8.9957 - val_loss: 8.2559\n",
      "Epoch 623/800\n",
      "8/8 [==============================] - 6s 782ms/step - loss: 8.7337 - val_loss: 8.9235\n",
      "Epoch 624/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 9.0479 - val_loss: 8.4115\n",
      "Epoch 625/800\n",
      "8/8 [==============================] - 6s 796ms/step - loss: 8.9523 - val_loss: 8.6783\n",
      "Epoch 626/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 9.0167 - val_loss: 8.5629\n",
      "Epoch 627/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 8.7331 - val_loss: 9.6941\n",
      "Epoch 628/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 6s 800ms/step - loss: 9.4340 - val_loss: 9.4007\n",
      "Epoch 629/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 9.3595 - val_loss: 9.1140\n",
      "Epoch 630/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 9.1594 - val_loss: 8.8447\n",
      "Epoch 631/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 8.8709 - val_loss: 10.4980\n",
      "Epoch 632/800\n",
      "8/8 [==============================] - 6s 806ms/step - loss: 9.2327 - val_loss: 11.9355\n",
      "Epoch 633/800\n",
      "8/8 [==============================] - 6s 807ms/step - loss: 8.7312 - val_loss: 11.1195\n",
      "Epoch 634/800\n",
      "8/8 [==============================] - 6s 795ms/step - loss: 8.9909 - val_loss: 9.9741\n",
      "Epoch 635/800\n",
      "8/8 [==============================] - 6s 797ms/step - loss: 8.6713 - val_loss: 8.3810\n",
      "Epoch 636/800\n",
      "8/8 [==============================] - 6s 796ms/step - loss: 9.2168 - val_loss: 10.5040\n",
      "Epoch 637/800\n",
      "8/8 [==============================] - 6s 807ms/step - loss: 9.1936 - val_loss: 9.0854\n",
      "Epoch 638/800\n",
      "8/8 [==============================] - 6s 803ms/step - loss: 8.6930 - val_loss: 11.4816\n",
      "Epoch 639/800\n",
      "8/8 [==============================] - 6s 805ms/step - loss: 8.9615 - val_loss: 13.8496\n",
      "Epoch 640/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 8.7476 - val_loss: 9.2856\n",
      "Epoch 641/800\n",
      "8/8 [==============================] - 6s 788ms/step - loss: 9.1008 - val_loss: 9.3168\n",
      "Epoch 642/800\n",
      "8/8 [==============================] - 6s 799ms/step - loss: 8.6924 - val_loss: 9.9558\n",
      "Epoch 643/800\n",
      "8/8 [==============================] - 6s 798ms/step - loss: 8.7608 - val_loss: 9.7682\n",
      "Epoch 644/800\n",
      "8/8 [==============================] - 6s 804ms/step - loss: 8.5731 - val_loss: 8.2341\n",
      "Epoch 645/800\n",
      "8/8 [==============================] - 6s 788ms/step - loss: 8.7966 - val_loss: 8.3749\n",
      "Epoch 646/800\n",
      "8/8 [==============================] - 6s 807ms/step - loss: 8.8952 - val_loss: 9.0319\n",
      "Epoch 647/800\n",
      "8/8 [==============================] - 6s 809ms/step - loss: 9.6777 - val_loss: 8.6085\n",
      "Epoch 648/800\n",
      "8/8 [==============================] - 6s 786ms/step - loss: 9.2714 - val_loss: 8.6766\n",
      "Epoch 649/800\n",
      "8/8 [==============================] - 6s 801ms/step - loss: 8.7109 - val_loss: 54.9166\n",
      "Epoch 650/800\n",
      "1/8 [==>...........................] - ETA: 4s - loss: 9.1259"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! python train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import colorsys\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "\n",
    "from yolo3.model import yolo_eval, yolo_body, tiny_yolo_body\n",
    "from yolo3.utils import letterbox_image\n",
    "import os\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "class YOLO(object):\n",
    "    _defaults = {\n",
    "        \"model_path\": 'logs/000/trained_weights.h5',\n",
    "        \"anchors_path\": 'model_data/yolo_anchors.txt',\n",
    "        \"classes_path\": 'model_data/voc_classes.txt',\n",
    "        \"score\" : 0.3,\n",
    "        \"iou\" : 0.45,\n",
    "        \"model_image_size\" : (416, 416),\n",
    "        \"gpu_num\" : 1,\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def get_defaults(cls, n):\n",
    "        if n in cls._defaults:\n",
    "            return cls._defaults[n]\n",
    "        else:\n",
    "            return \"Unrecognized attribute name '\" + n + \"'\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(self._defaults) # set up default values\n",
    "        self.__dict__.update(kwargs) # and update with user overrides\n",
    "        self.class_names = self._get_class()\n",
    "        self.anchors = self._get_anchors()\n",
    "        self.sess = K.get_session()\n",
    "        self.boxes, self.scores, self.classes = self.generate()\n",
    "\n",
    "    def _get_class(self):\n",
    "        classes_path = os.path.expanduser(self.classes_path)\n",
    "        with open(classes_path) as f:\n",
    "            class_names = f.readlines()\n",
    "        class_names = [c.strip() for c in class_names]\n",
    "        return class_names\n",
    "\n",
    "    def _get_anchors(self):\n",
    "        anchors_path = os.path.expanduser(self.anchors_path)\n",
    "        with open(anchors_path) as f:\n",
    "            anchors = f.readline()\n",
    "        anchors = [float(x) for x in anchors.split(',')]\n",
    "        return np.array(anchors).reshape(-1, 2)\n",
    "\n",
    "    def generate(self):\n",
    "        model_path = os.path.expanduser(self.model_path)\n",
    "        assert model_path.endswith('.h5'), 'Keras model or weights must be a .h5 file.'\n",
    "\n",
    "        # Load model, or construct model and load weights.\n",
    "        num_anchors = len(self.anchors)\n",
    "        num_classes = len(self.class_names)\n",
    "        is_tiny_version = num_anchors==6 # default setting\n",
    "        try:\n",
    "            self.yolo_model = load_model(model_path, compile=False)\n",
    "        except:\n",
    "            self.yolo_model = tiny_yolo_body(Input(shape=(None,None,3)), num_anchors//2, num_classes) \\\n",
    "                if is_tiny_version else yolo_body(Input(shape=(None,None,3)), num_anchors//3, num_classes)\n",
    "            self.yolo_model.load_weights(self.model_path) # make sure model, anchors and classes match\n",
    "        else:\n",
    "            assert self.yolo_model.layers[-1].output_shape[-1] == \\\n",
    "                num_anchors/len(self.yolo_model.output) * (num_classes + 5), \\\n",
    "                'Mismatch between model and given anchor and class sizes'\n",
    "\n",
    "        print('{} model, anchors, and classes loaded.'.format(model_path))\n",
    "\n",
    "        # Generate colors for drawing bounding boxes.\n",
    "        hsv_tuples = [(x / len(self.class_names), 1., 1.)\n",
    "                      for x in range(len(self.class_names))]\n",
    "        self.colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
    "        self.colors = list(\n",
    "            map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)),\n",
    "                self.colors))\n",
    "        np.random.seed(10101)  # Fixed seed for consistent colors across runs.\n",
    "        np.random.shuffle(self.colors)  # Shuffle colors to decorrelate adjacent classes.\n",
    "        np.random.seed(None)  # Reset seed to default.\n",
    "\n",
    "        # Generate output tensor targets for filtered bounding boxes.\n",
    "        self.input_image_shape = K.placeholder(shape=(2, ))\n",
    "        if self.gpu_num>=2:\n",
    "            self.yolo_model = multi_gpu_model(self.yolo_model, gpus=self.gpu_num)\n",
    "        boxes, scores, classes = yolo_eval(self.yolo_model.output, self.anchors,\n",
    "                len(self.class_names), self.input_image_shape,\n",
    "                score_threshold=self.score, iou_threshold=self.iou)\n",
    "        return boxes, scores, classes\n",
    "\n",
    "    def detect_image(self, image):\n",
    "        start = timer()\n",
    "\n",
    "        if self.model_image_size != (None, None):\n",
    "            assert self.model_image_size[0]%32 == 0, 'Multiples of 32 required'\n",
    "            assert self.model_image_size[1]%32 == 0, 'Multiples of 32 required'\n",
    "            boxed_image = letterbox_image(image, tuple(reversed(self.model_image_size)))\n",
    "        else:\n",
    "            new_image_size = (image.width - (image.width % 32),\n",
    "                              image.height - (image.height % 32))\n",
    "            boxed_image = letterbox_image(image, new_image_size)\n",
    "        image_data = np.array(boxed_image, dtype='float32')\n",
    "\n",
    "        print(image_data.shape)\n",
    "        image_data /= 255.\n",
    "        image_data = np.expand_dims(image_data, 0)  # Add batch dimension.\n",
    "\n",
    "        out_boxes, out_scores, out_classes = self.sess.run(\n",
    "            [self.boxes, self.scores, self.classes],\n",
    "            feed_dict={\n",
    "                self.yolo_model.input: image_data,\n",
    "                self.input_image_shape: [image.size[1], image.size[0]],\n",
    "                K.learning_phase(): 0\n",
    "            })\n",
    "\n",
    "        print('Found {} boxes for {}'.format(len(out_boxes), 'img'))\n",
    "\n",
    "        font = ImageFont.truetype(font='font/FiraMono-Medium.otf',\n",
    "                    size=np.floor(3e-2 * image.size[1] + 0.5).astype('int32'))\n",
    "        thickness = (image.size[0] + image.size[1]) // 300\n",
    "\n",
    "        for i, c in reversed(list(enumerate(out_classes))):\n",
    "            predicted_class = self.class_names[c]\n",
    "            box = out_boxes[i]\n",
    "            score = out_scores[i]\n",
    "\n",
    "            label = '{} {:.2f}'.format(predicted_class, score)\n",
    "            draw = ImageDraw.Draw(image)\n",
    "            label_size = draw.textsize(label, font)\n",
    "\n",
    "            top, left, bottom, right = box\n",
    "            top = max(0, np.floor(top + 0.5).astype('int32'))\n",
    "            left = max(0, np.floor(left + 0.5).astype('int32'))\n",
    "            bottom = min(image.size[1], np.floor(bottom + 0.5).astype('int32'))\n",
    "            right = min(image.size[0], np.floor(right + 0.5).astype('int32'))\n",
    "            print(label, (left, top), (right, bottom))\n",
    "\n",
    "            if top - label_size[1] >= 0:\n",
    "                text_origin = np.array([left, top - label_size[1]])\n",
    "            else:\n",
    "                text_origin = np.array([left, top + 1])\n",
    "\n",
    "            # My kingdom for a good redistributable image drawing library.\n",
    "            for i in range(thickness):\n",
    "                draw.rectangle(\n",
    "                    [left + i, top + i, right - i, bottom - i],\n",
    "                    outline=self.colors[c])\n",
    "            draw.rectangle(\n",
    "                [tuple(text_origin), tuple(text_origin + label_size)],\n",
    "                fill=self.colors[c])\n",
    "            draw.text(text_origin, label, fill=(0, 0, 0), font=font)\n",
    "            del draw\n",
    "\n",
    "        end = timer()\n",
    "        print(end - start)\n",
    "        return image\n",
    "\n",
    "    def close_session(self):\n",
    "        self.sess.close()\n",
    "\n",
    "def detect_video(yolo, video_path, output_path=\"\"):\n",
    "    import cv2\n",
    "    vid = cv2.VideoCapture(video_path)\n",
    "    if not vid.isOpened():\n",
    "        raise IOError(\"Couldn't open webcam or video\")\n",
    "    video_FourCC    = int(vid.get(cv2.CAP_PROP_FOURCC))\n",
    "    video_fps       = vid.get(cv2.CAP_PROP_FPS)\n",
    "    video_size      = (int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "                        int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "    isOutput = True if output_path != \"\" else False\n",
    "    if isOutput:\n",
    "        print(\"!!! TYPE:\", type(output_path), type(video_FourCC), type(video_fps), type(video_size))\n",
    "        out = cv2.VideoWriter(output_path, video_FourCC, video_fps, video_size)\n",
    "    accum_time = 0\n",
    "    curr_fps = 0\n",
    "    fps = \"FPS: ??\"\n",
    "    prev_time = timer()\n",
    "    while True:\n",
    "        return_value, frame = vid.read()\n",
    "        image = Image.fromarray(frame)\n",
    "        image = yolo.detect_image(image)\n",
    "        result = np.asarray(image)\n",
    "        curr_time = timer()\n",
    "        exec_time = curr_time - prev_time\n",
    "        prev_time = curr_time\n",
    "        accum_time = accum_time + exec_time\n",
    "        curr_fps = curr_fps + 1\n",
    "        if accum_time > 1:\n",
    "            accum_time = accum_time - 1\n",
    "            fps = \"FPS: \" + str(curr_fps)\n",
    "            curr_fps = 0\n",
    "        cv2.putText(result, text=fps, org=(3, 15), fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    fontScale=0.50, color=(255, 0, 0), thickness=2)\n",
    "        cv2.namedWindow(\"result\", cv2.WINDOW_NORMAL)\n",
    "        cv2.imshow(\"result\", result)\n",
    "        if isOutput:\n",
    "            out.write(result)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    yolo.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Image detection mode\n",
      " Ignoring remaining command line arguments: ./path2your_video,\n",
      "2019-01-03 02:27:07.255675: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-01-03 02:27:07.392888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: \n",
      "name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.8225\n",
      "pciBusID: 0000:01:00.0\n",
      "totalMemory: 7.93GiB freeMemory: 285.19MiB\n",
      "2019-01-03 02:27:07.503880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 1 with properties: \n",
      "name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.8225\n",
      "pciBusID: 0000:02:00.0\n",
      "totalMemory: 7.93GiB freeMemory: 285.56MiB\n",
      "2019-01-03 02:27:07.504026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0, 1\n",
      "2019-01-03 02:27:07.950570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-01-03 02:27:07.950618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 1 \n",
      "2019-01-03 02:27:07.950630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N Y \n",
      "2019-01-03 02:27:07.950639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1:   Y N \n",
      "2019-01-03 02:27:07.950839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "2019-01-03 02:27:07.951639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 0 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)\n",
      "2019-01-03 02:27:26.740571: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4B.  Current allocation summary follows.\n",
      "2019-01-03 02:27:26.740684: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256): \tTotal Chunks: 157, Chunks in use: 157. 39.2KiB allocated for chunks. 39.2KiB in use in bin. 3.5KiB client-requested in use in bin.\n",
      "2019-01-03 02:27:26.740762: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (512): \tTotal Chunks: 28, Chunks in use: 28. 14.0KiB allocated for chunks. 14.0KiB in use in bin. 14.0KiB client-requested in use in bin.\n",
      "2019-01-03 02:27:26.740807: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1024): \tTotal Chunks: 47, Chunks in use: 47. 47.2KiB allocated for chunks. 47.2KiB in use in bin. 47.0KiB client-requested in use in bin.\n",
      "2019-01-03 02:27:26.740851: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2048): \tTotal Chunks: 41, Chunks in use: 41. 83.5KiB allocated for chunks. 83.5KiB in use in bin. 83.4KiB client-requested in use in bin.\n",
      "2019-01-03 02:27:26.740893: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4096): \tTotal Chunks: 2, Chunks in use: 2. 8.0KiB allocated for chunks. 8.0KiB in use in bin. 8.0KiB client-requested in use in bin.\n",
      "2019-01-03 02:27:26.740932: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2019-01-03 02:27:26.740970: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2019-01-03 02:27:26.741009: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2019-01-03 02:27:26.741049: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2019-01-03 02:27:26.741090: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2019-01-03 02:27:26.741128: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2019-01-03 02:27:26.741166: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2019-01-03 02:27:26.741205: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2019-01-03 02:27:26.741244: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2019-01-03 02:27:26.741283: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2019-01-03 02:27:26.741320: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2019-01-03 02:27:26.741357: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2019-01-03 02:27:26.741394: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2019-01-03 02:27:26.741431: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2019-01-03 02:27:26.741468: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2019-01-03 02:27:26.741627: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2019-01-03 02:27:26.741669: I tensorflow/core/common_runtime/bfc_allocator.cc:646] Bin for 256B was 256B, Chunk State: \n",
      "2019-01-03 02:27:26.741703: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13800000 of size 1280\n",
      "2019-01-03 02:27:26.741730: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13800500 of size 256\n",
      "2019-01-03 02:27:26.741766: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13800600 of size 256\n",
      "2019-01-03 02:27:26.741792: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13800700 of size 256\n",
      "2019-01-03 02:27:26.741817: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13800800 of size 256\n",
      "2019-01-03 02:27:26.741841: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13800900 of size 256\n",
      "2019-01-03 02:27:26.741866: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13800a00 of size 256\n",
      "2019-01-03 02:27:26.741890: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13800b00 of size 256\n",
      "2019-01-03 02:27:26.741915: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13800c00 of size 256\n",
      "2019-01-03 02:27:26.741940: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13800d00 of size 3584\n",
      "2019-01-03 02:27:26.741964: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13801b00 of size 256\n",
      "2019-01-03 02:27:26.741988: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13801c00 of size 256\n",
      "2019-01-03 02:27:26.742013: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13801d00 of size 256\n",
      "2019-01-03 02:27:26.742037: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13801e00 of size 256\n",
      "2019-01-03 02:27:26.742063: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13801f00 of size 1024\n",
      "2019-01-03 02:27:26.742087: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13802300 of size 256\n",
      "2019-01-03 02:27:26.742111: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13802400 of size 1024\n",
      "2019-01-03 02:27:26.742135: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13802800 of size 256\n",
      "2019-01-03 02:27:26.742160: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13802900 of size 512\n",
      "2019-01-03 02:27:26.742185: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13802b00 of size 256\n",
      "2019-01-03 02:27:26.742209: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13802c00 of size 512\n",
      "2019-01-03 02:27:26.742233: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13802e00 of size 256\n",
      "2019-01-03 02:27:26.742257: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13802f00 of size 1024\n",
      "2019-01-03 02:27:26.742281: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13803300 of size 256\n",
      "2019-01-03 02:27:26.742306: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13803400 of size 1024\n",
      "2019-01-03 02:27:26.742330: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13803800 of size 256\n",
      "2019-01-03 02:27:26.742354: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13803900 of size 512\n",
      "2019-01-03 02:27:26.742378: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13803b00 of size 256\n",
      "2019-01-03 02:27:26.742403: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13803c00 of size 512\n",
      "2019-01-03 02:27:26.742434: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13803e00 of size 256\n",
      "2019-01-03 02:27:26.742460: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13803f00 of size 1024\n",
      "2019-01-03 02:27:26.742484: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13804300 of size 256\n",
      "2019-01-03 02:27:26.742518: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13804400 of size 1024\n",
      "2019-01-03 02:27:26.742544: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13804800 of size 256\n",
      "2019-01-03 02:27:26.742568: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13804900 of size 512\n",
      "2019-01-03 02:27:26.742592: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13804b00 of size 256\n",
      "2019-01-03 02:27:26.742616: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13804c00 of size 512\n",
      "2019-01-03 02:27:26.742640: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13804e00 of size 256\n",
      "2019-01-03 02:27:26.742670: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13804f00 of size 1024\n",
      "2019-01-03 02:27:26.742695: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13805300 of size 256\n",
      "2019-01-03 02:27:26.742719: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13805400 of size 1024\n",
      "2019-01-03 02:27:26.742743: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13805800 of size 256\n",
      "2019-01-03 02:27:26.742768: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13805900 of size 512\n",
      "2019-01-03 02:27:26.742792: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13805b00 of size 256\n",
      "2019-01-03 02:27:26.742816: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13805c00 of size 512\n",
      "2019-01-03 02:27:26.742840: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13805e00 of size 256\n",
      "2019-01-03 02:27:26.742864: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13805f00 of size 1024\n",
      "2019-01-03 02:27:26.742888: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13806300 of size 256\n",
      "2019-01-03 02:27:26.742913: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13806400 of size 1024\n",
      "2019-01-03 02:27:26.742938: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13806800 of size 256\n",
      "2019-01-03 02:27:26.742962: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13806900 of size 512\n",
      "2019-01-03 02:27:26.742986: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13806b00 of size 256\n",
      "2019-01-03 02:27:26.743010: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13806c00 of size 512\n",
      "2019-01-03 02:27:26.743033: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13806e00 of size 256\n",
      "2019-01-03 02:27:26.743057: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13806f00 of size 256\n",
      "2019-01-03 02:27:26.743081: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13807000 of size 256\n",
      "2019-01-03 02:27:26.743105: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13807100 of size 256\n",
      "2019-01-03 02:27:26.743129: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13807200 of size 256\n",
      "2019-01-03 02:27:26.743155: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13807300 of size 1024\n",
      "2019-01-03 02:27:26.743179: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13807700 of size 256\n",
      "2019-01-03 02:27:26.743203: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13807800 of size 1024\n",
      "2019-01-03 02:27:26.743227: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13807c00 of size 256\n",
      "2019-01-03 02:27:26.743251: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13807d00 of size 512\n",
      "2019-01-03 02:27:26.743275: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13807f00 of size 256\n",
      "2019-01-03 02:27:26.743299: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13808000 of size 512\n",
      "2019-01-03 02:27:26.743323: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13808200 of size 256\n",
      "2019-01-03 02:27:26.743347: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13808300 of size 1024\n",
      "2019-01-03 02:27:26.743372: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13808700 of size 256\n",
      "2019-01-03 02:27:26.743397: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13808800 of size 1024\n",
      "2019-01-03 02:27:26.743421: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13808c00 of size 256\n",
      "2019-01-03 02:27:26.743445: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13808d00 of size 512\n",
      "2019-01-03 02:27:26.743469: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13808f00 of size 256\n",
      "2019-01-03 02:27:26.743493: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13809000 of size 512\n",
      "2019-01-03 02:27:26.743525: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13809200 of size 256\n",
      "2019-01-03 02:27:26.743549: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13809300 of size 1024\n",
      "2019-01-03 02:27:26.743573: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13809700 of size 256\n",
      "2019-01-03 02:27:26.743598: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13809800 of size 1024\n",
      "2019-01-03 02:27:26.743623: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13809c00 of size 256\n",
      "2019-01-03 02:27:26.743647: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13809d00 of size 512\n",
      "2019-01-03 02:27:26.743671: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13809f00 of size 256\n",
      "2019-01-03 02:27:26.743695: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1380a000 of size 512\n",
      "2019-01-03 02:27:26.743719: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1380a200 of size 256\n",
      "2019-01-03 02:27:26.743743: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1380a300 of size 1024\n",
      "2019-01-03 02:27:26.743767: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1380a700 of size 256\n",
      "2019-01-03 02:27:26.743792: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1380a800 of size 1024\n",
      "2019-01-03 02:27:26.743817: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1380ac00 of size 256\n",
      "2019-01-03 02:27:26.743842: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1380ad00 of size 2048\n",
      "2019-01-03 02:27:26.743866: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1380b500 of size 256\n",
      "2019-01-03 02:27:26.743891: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1380b600 of size 2048\n",
      "2019-01-03 02:27:26.743915: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1380be00 of size 256\n",
      "2019-01-03 02:27:26.743939: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1380bf00 of size 1024\n",
      "2019-01-03 02:27:26.743963: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1380c300 of size 256\n",
      "2019-01-03 02:27:26.743987: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1380c400 of size 1024\n",
      "2019-01-03 02:27:26.744011: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1380c800 of size 256\n",
      "2019-01-03 02:27:26.744036: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1380c900 of size 2048\n",
      "2019-01-03 02:27:26.744065: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1380d100 of size 256\n",
      "2019-01-03 02:27:26.744090: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1380d200 of size 2048\n",
      "2019-01-03 02:27:26.744114: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1380da00 of size 256\n",
      "2019-01-03 02:27:26.744138: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1380db00 of size 256\n",
      "2019-01-03 02:27:26.744162: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1380dc00 of size 256\n",
      "2019-01-03 02:27:26.744187: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1380dd00 of size 256\n",
      "2019-01-03 02:27:26.744211: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1380de00 of size 256\n",
      "2019-01-03 02:27:26.744235: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1380df00 of size 256\n",
      "2019-01-03 02:27:26.744259: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1380e000 of size 256\n",
      "2019-01-03 02:27:26.744283: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1380e100 of size 1024\n",
      "2019-01-03 02:27:26.744308: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1380e500 of size 256\n",
      "2019-01-03 02:27:26.744332: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1380e600 of size 1024\n",
      "2019-01-03 02:27:26.744356: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1380ea00 of size 256\n",
      "2019-01-03 02:27:26.744381: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1380eb00 of size 2048\n",
      "2019-01-03 02:27:26.744406: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1380f300 of size 256\n",
      "2019-01-03 02:27:26.744430: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1380f400 of size 2048\n",
      "2019-01-03 02:27:26.744454: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1380fc00 of size 256\n",
      "2019-01-03 02:27:26.744478: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1380fd00 of size 1024\n",
      "2019-01-03 02:27:26.744513: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13810100 of size 256\n",
      "2019-01-03 02:27:26.744556: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13810200 of size 1024\n",
      "2019-01-03 02:27:26.744586: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13810600 of size 256\n",
      "2019-01-03 02:27:26.744612: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13810700 of size 2048\n",
      "2019-01-03 02:27:26.744636: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13810f00 of size 256\n",
      "2019-01-03 02:27:26.744661: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13811000 of size 2048\n",
      "2019-01-03 02:27:26.744685: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13811800 of size 256\n",
      "2019-01-03 02:27:26.744710: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13811900 of size 1024\n",
      "2019-01-03 02:27:26.744735: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13811d00 of size 256\n",
      "2019-01-03 02:27:26.744760: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13811e00 of size 1024\n",
      "2019-01-03 02:27:26.744784: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13812200 of size 256\n",
      "2019-01-03 02:27:26.744808: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13812300 of size 2048\n",
      "2019-01-03 02:27:26.744833: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13812b00 of size 256\n",
      "2019-01-03 02:27:26.744857: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13812c00 of size 2048\n",
      "2019-01-03 02:27:26.744882: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13813400 of size 256\n",
      "2019-01-03 02:27:26.744906: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13813500 of size 1024\n",
      "2019-01-03 02:27:26.744931: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13813900 of size 256\n",
      "2019-01-03 02:27:26.744961: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13813a00 of size 1024\n",
      "2019-01-03 02:27:26.744987: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13813e00 of size 256\n",
      "2019-01-03 02:27:26.745012: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13813f00 of size 2048\n",
      "2019-01-03 02:27:26.745036: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13814700 of size 256\n",
      "2019-01-03 02:27:26.745061: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13814800 of size 2048\n",
      "2019-01-03 02:27:26.745086: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13815000 of size 256\n",
      "2019-01-03 02:27:26.745110: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13815100 of size 1024\n",
      "2019-01-03 02:27:26.745135: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13815500 of size 256\n",
      "2019-01-03 02:27:26.745159: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13815600 of size 1024\n",
      "2019-01-03 02:27:26.745184: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13815a00 of size 256\n",
      "2019-01-03 02:27:26.745213: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13815b00 of size 2048\n",
      "2019-01-03 02:27:26.745238: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13816300 of size 256\n",
      "2019-01-03 02:27:26.745262: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13816400 of size 2048\n",
      "2019-01-03 02:27:26.745287: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13816c00 of size 256\n",
      "2019-01-03 02:27:26.745311: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13816d00 of size 256\n",
      "2019-01-03 02:27:26.745335: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13816e00 of size 256\n",
      "2019-01-03 02:27:26.745359: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13816f00 of size 256\n",
      "2019-01-03 02:27:26.745383: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13817000 of size 256\n",
      "2019-01-03 02:27:26.745407: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13817100 of size 1024\n",
      "2019-01-03 02:27:26.745436: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13817500 of size 256\n",
      "2019-01-03 02:27:26.745463: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13817600 of size 1024\n",
      "2019-01-03 02:27:26.745487: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13817a00 of size 256\n",
      "2019-01-03 02:27:26.745520: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13817b00 of size 2048\n",
      "2019-01-03 02:27:26.745545: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13818300 of size 256\n",
      "2019-01-03 02:27:26.745569: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13818400 of size 2048\n",
      "2019-01-03 02:27:26.745593: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13818c00 of size 256\n",
      "2019-01-03 02:27:26.745617: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13818d00 of size 1024\n",
      "2019-01-03 02:27:26.745641: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13819100 of size 256\n",
      "2019-01-03 02:27:26.745666: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13819200 of size 1024\n",
      "2019-01-03 02:27:26.745692: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13819600 of size 256\n",
      "2019-01-03 02:27:26.745716: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13819700 of size 2048\n",
      "2019-01-03 02:27:26.745741: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13819f00 of size 256\n",
      "2019-01-03 02:27:26.745765: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1381a000 of size 2048\n",
      "2019-01-03 02:27:26.745790: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1381a800 of size 256\n",
      "2019-01-03 02:27:26.745814: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1381a900 of size 256\n",
      "2019-01-03 02:27:26.745838: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1381aa00 of size 256\n",
      "2019-01-03 02:27:26.745862: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1381ab00 of size 2048\n",
      "2019-01-03 02:27:26.745887: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1381b300 of size 256\n",
      "2019-01-03 02:27:26.745918: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1381b400 of size 2048\n",
      "2019-01-03 02:27:26.745943: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1381bc00 of size 256\n",
      "2019-01-03 02:27:26.745967: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1381bd00 of size 256\n",
      "2019-01-03 02:27:26.745991: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1381be00 of size 256\n",
      "2019-01-03 02:27:26.746015: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1381bf00 of size 2048\n",
      "2019-01-03 02:27:26.746039: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1381c700 of size 256\n",
      "2019-01-03 02:27:26.746064: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1381c800 of size 2048\n",
      "2019-01-03 02:27:26.746089: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1381d000 of size 256\n",
      "2019-01-03 02:27:26.746114: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1381d100 of size 256\n",
      "2019-01-03 02:27:26.746138: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1381d200 of size 256\n",
      "2019-01-03 02:27:26.746168: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1381d300 of size 2048\n",
      "2019-01-03 02:27:26.746194: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1381db00 of size 256\n",
      "2019-01-03 02:27:26.746218: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1381dc00 of size 2048\n",
      "2019-01-03 02:27:26.746242: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1381e400 of size 256\n",
      "2019-01-03 02:27:26.746267: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1381e500 of size 512\n",
      "2019-01-03 02:27:26.746291: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1381e700 of size 256\n",
      "2019-01-03 02:27:26.746315: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1381e800 of size 512\n",
      "2019-01-03 02:27:26.746339: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1381ea00 of size 256\n",
      "2019-01-03 02:27:26.746363: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1381eb00 of size 256\n",
      "2019-01-03 02:27:26.746394: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1381ec00 of size 256\n",
      "2019-01-03 02:27:26.746419: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1381ed00 of size 2048\n",
      "2019-01-03 02:27:26.746444: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1381f500 of size 256\n",
      "2019-01-03 02:27:26.746468: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1381f600 of size 2048\n",
      "2019-01-03 02:27:26.746493: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1381fe00 of size 256\n",
      "2019-01-03 02:27:26.746523: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1381ff00 of size 256\n",
      "2019-01-03 02:27:26.746548: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13820000 of size 256\n",
      "2019-01-03 02:27:26.746572: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13820100 of size 2048\n",
      "2019-01-03 02:27:26.746597: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13820900 of size 256\n",
      "2019-01-03 02:27:26.746621: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13820a00 of size 2048\n",
      "2019-01-03 02:27:26.746651: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13821200 of size 256\n",
      "2019-01-03 02:27:26.746676: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13821300 of size 256\n",
      "2019-01-03 02:27:26.746700: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13821400 of size 256\n",
      "2019-01-03 02:27:26.746724: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13821500 of size 2048\n",
      "2019-01-03 02:27:26.746748: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13821d00 of size 256\n",
      "2019-01-03 02:27:26.746772: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13821e00 of size 2048\n",
      "2019-01-03 02:27:26.746797: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13822600 of size 256\n",
      "2019-01-03 02:27:26.746821: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13822700 of size 256\n",
      "2019-01-03 02:27:26.746845: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13822800 of size 256\n",
      "2019-01-03 02:27:26.746869: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13822900 of size 2048\n",
      "2019-01-03 02:27:26.746898: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13823100 of size 256\n",
      "2019-01-03 02:27:26.746924: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13823200 of size 2048\n",
      "2019-01-03 02:27:26.746954: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13823a00 of size 256\n",
      "2019-01-03 02:27:26.746978: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13823b00 of size 256\n",
      "2019-01-03 02:27:26.747003: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13823c00 of size 4096\n",
      "2019-01-03 02:27:26.747027: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13824c00 of size 4096\n",
      "2019-01-03 02:27:26.747051: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13825c00 of size 256\n",
      "2019-01-03 02:27:26.747076: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13825d00 of size 1024\n",
      "2019-01-03 02:27:26.747100: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13826100 of size 256\n",
      "2019-01-03 02:27:26.747124: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13826200 of size 1024\n",
      "2019-01-03 02:27:26.747149: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13826600 of size 256\n",
      "2019-01-03 02:27:26.747174: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13826700 of size 256\n",
      "2019-01-03 02:27:26.747198: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13826800 of size 256\n",
      "2019-01-03 02:27:26.747222: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13826900 of size 256\n",
      "2019-01-03 02:27:26.747246: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13826a00 of size 256\n",
      "2019-01-03 02:27:26.747270: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13826b00 of size 1024\n",
      "2019-01-03 02:27:26.747294: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13826f00 of size 256\n",
      "2019-01-03 02:27:26.747318: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13827000 of size 1024\n",
      "2019-01-03 02:27:26.747343: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13827400 of size 256\n",
      "2019-01-03 02:27:26.747373: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13827500 of size 2048\n",
      "2019-01-03 02:27:26.747397: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13827d00 of size 256\n",
      "2019-01-03 02:27:26.747422: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13827e00 of size 2048\n",
      "2019-01-03 02:27:26.747446: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13828600 of size 256\n",
      "2019-01-03 02:27:26.747470: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13828700 of size 1024\n",
      "2019-01-03 02:27:26.747494: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13828b00 of size 256\n",
      "2019-01-03 02:27:26.747525: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13828c00 of size 1024\n",
      "2019-01-03 02:27:26.747549: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13829000 of size 256\n",
      "2019-01-03 02:27:26.747574: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13829100 of size 2048\n",
      "2019-01-03 02:27:26.747598: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13829900 of size 256\n",
      "2019-01-03 02:27:26.747624: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce13829a00 of size 2048\n",
      "2019-01-03 02:27:26.747648: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382a200 of size 256\n",
      "2019-01-03 02:27:26.747673: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382a300 of size 1024\n",
      "2019-01-03 02:27:26.747697: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382a700 of size 256\n",
      "2019-01-03 02:27:26.747722: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382a800 of size 1024\n",
      "2019-01-03 02:27:26.747750: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382ac00 of size 256\n",
      "2019-01-03 02:27:26.747774: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382ad00 of size 2048\n",
      "2019-01-03 02:27:26.747799: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382b500 of size 2048\n",
      "2019-01-03 02:27:26.747823: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382bd00 of size 256\n",
      "2019-01-03 02:27:26.747853: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382be00 of size 2048\n",
      "2019-01-03 02:27:26.747878: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382c600 of size 2048\n",
      "2019-01-03 02:27:26.747902: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382ce00 of size 256\n",
      "2019-01-03 02:27:26.747927: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382cf00 of size 512\n",
      "2019-01-03 02:27:26.747951: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382d100 of size 256\n",
      "2019-01-03 02:27:26.747975: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382d200 of size 512\n",
      "2019-01-03 02:27:26.748000: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382d400 of size 256\n",
      "2019-01-03 02:27:26.748025: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382d500 of size 512\n",
      "2019-01-03 02:27:26.748049: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382d700 of size 256\n",
      "2019-01-03 02:27:26.748073: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382d800 of size 512\n",
      "2019-01-03 02:27:26.748098: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382da00 of size 256\n",
      "2019-01-03 02:27:26.748122: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382db00 of size 1024\n",
      "2019-01-03 02:27:26.748146: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382df00 of size 256\n",
      "2019-01-03 02:27:26.748170: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382e000 of size 1024\n",
      "2019-01-03 02:27:26.748195: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382e400 of size 256\n",
      "2019-01-03 02:27:26.748219: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382e500 of size 512\n",
      "2019-01-03 02:27:26.748244: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382e700 of size 256\n",
      "2019-01-03 02:27:26.748268: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382e800 of size 512\n",
      "2019-01-03 02:27:26.748293: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382ea00 of size 256\n",
      "2019-01-03 02:27:26.748320: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382eb00 of size 512\n",
      "2019-01-03 02:27:26.748344: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382ed00 of size 256\n",
      "2019-01-03 02:27:26.748368: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382ee00 of size 512\n",
      "2019-01-03 02:27:26.748392: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382f000 of size 256\n",
      "2019-01-03 02:27:26.748416: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382f100 of size 1024\n",
      "2019-01-03 02:27:26.748441: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382f500 of size 256\n",
      "2019-01-03 02:27:26.748465: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382f600 of size 1024\n",
      "2019-01-03 02:27:26.748489: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382fa00 of size 256\n",
      "2019-01-03 02:27:26.748529: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382fb00 of size 512\n",
      "2019-01-03 02:27:26.748556: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382fd00 of size 256\n",
      "2019-01-03 02:27:26.748580: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fce1382fe00 of size 512\n",
      "2019-01-03 02:27:26.748604: I tensorflow/core/common_runtime/bfc_allocator.cc:671]      Summary of in-use Chunks by size: \n",
      "2019-01-03 02:27:26.748634: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 157 Chunks of size 256 totalling 39.2KiB\n",
      "2019-01-03 02:27:26.748662: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 28 Chunks of size 512 totalling 14.0KiB\n",
      "2019-01-03 02:27:26.748689: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 46 Chunks of size 1024 totalling 46.0KiB\n",
      "2019-01-03 02:27:26.748716: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2019-01-03 02:27:26.748743: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 40 Chunks of size 2048 totalling 80.0KiB\n",
      "2019-01-03 02:27:26.748775: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 3584 totalling 3.5KiB\n",
      "2019-01-03 02:27:26.748802: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 4096 totalling 8.0KiB\n",
      "2019-01-03 02:27:26.748829: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Sum Total of in-use chunks: 192.0KiB\n",
      "2019-01-03 02:27:26.748860: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats: \n",
      "Limit:                      196608\n",
      "InUse:                      196608\n",
      "MaxInUse:                   196608\n",
      "NumAllocs:                     275\n",
      "MaxAllocSize:                 4096\n",
      "\n",
      "2019-01-03 02:27:26.748965: W tensorflow/core/common_runtime/bfc_allocator.cc:279] ****************************************************************************************************\n",
      "2019-01-03 02:27:26.749030: W tensorflow/core/framework/op_kernel.cc:1295] OP_REQUIRES failed at constant_op.cc:75 : Resource exhausted: OOM when allocating tensor of shape [] and type float\n",
      "2019-01-03 02:27:26.749131: E tensorflow/core/common_runtime/executor.cc:696] Executor failed to create kernel. Resource exhausted: OOM when allocating tensor of shape [] and type float\n",
      "\t [[Node: batch_normalization_71/moving_variance/local_step/Initializer/zeros = Const[_class=[\"loc:@batch_normalization_71/moving_variance/Assign\"], dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 0>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n",
      "Traceback (most recent call last):\n",
      "  File \"/notebooks/attention/keras-yolo3-master/yolo.py\", line 70, in generate\n",
      "    self.yolo_model = load_model(model_path, compile=False)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/saving.py\", line 259, in load_model\n",
      "    raise ValueError('No model found in config file.')\n",
      "ValueError: No model found in config file.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1322, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1307, in _run_fn\n",
      "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1409, in _call_tf_sessionrun\n",
      "    run_metadata)\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor of shape [] and type float\n",
      "\t [[Node: batch_normalization_71/moving_variance/local_step/Initializer/zeros = Const[_class=[\"loc:@batch_normalization_71/moving_variance/Assign\"], dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 0>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"yolo_video.py\", line 73, in <module>\n",
      "    detect_img(YOLO(**vars(FLAGS)))\n",
      "  File \"/notebooks/attention/keras-yolo3-master/yolo.py\", line 45, in __init__\n",
      "    self.boxes, self.scores, self.classes = self.generate()\n",
      "  File \"/notebooks/attention/keras-yolo3-master/yolo.py\", line 74, in generate\n",
      "    self.yolo_model.load_weights(self.model_path) # make sure model, anchors and classes match\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/network.py\", line 1180, in load_weights\n",
      "    f, self.layers, reshape=reshape)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/saving.py\", line 929, in load_weights_from_hdf5_group\n",
      "    K.batch_set_value(weight_value_tuples)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\", line 2435, in batch_set_value\n",
      "    get_session().run(assign_ops, feed_dict=feed_dict)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\", line 203, in get_session\n",
      "    session.run(tf.variables_initializer(uninitialized_vars))\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 900, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1135, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1316, in _do_run\n",
      "    run_metadata)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1335, in _do_call\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    raise type(e)(node_def, op, message)\r\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor of shape [] and type float\r\n",
      "\t [[Node: batch_normalization_71/moving_variance/local_step/Initializer/zeros = Const[_class=[\"loc:@batch_normalization_71/moving_variance/Assign\"], dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 0>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n",
      "\r\n",
      "Caused by op 'batch_normalization_71/moving_variance/local_step/Initializer/zeros', defined at:\r\n",
      "  File \"yolo_video.py\", line 73, in <module>\r\n",
      "    detect_img(YOLO(**vars(FLAGS)))\r\n",
      "  File \"/notebooks/attention/keras-yolo3-master/yolo.py\", line 45, in __init__\r\n",
      "    self.boxes, self.scores, self.classes = self.generate()\r\n",
      "  File \"/notebooks/attention/keras-yolo3-master/yolo.py\", line 73, in generate\r\n",
      "    if is_tiny_version else yolo_body(Input(shape=(None,None,3)), num_anchors//3, num_classes)\r\n",
      "  File \"/notebooks/attention/keras-yolo3-master/yolo3/model.py\", line 85, in yolo_body\r\n",
      "    x, y3 = make_last_layers(x, 128, num_anchors*(num_classes+5))\r\n",
      "  File \"/notebooks/attention/keras-yolo3-master/yolo3/model.py\", line 63, in make_last_layers\r\n",
      "    DarknetConv2D_BN_Leaky(num_filters, (1,1)))(x)\r\n",
      "  File \"/notebooks/attention/keras-yolo3-master/yolo3/utils.py\", line 16, in <lambda>\r\n",
      "    return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)\r\n",
      "  File \"/notebooks/attention/keras-yolo3-master/yolo3/utils.py\", line 16, in <lambda>\r\n",
      "    return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)\r\n",
      "  File \"/notebooks/attention/keras-yolo3-master/yolo3/utils.py\", line 16, in <lambda>\r\n",
      "    return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)\r\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/base_layer.py\", line 460, in __call__\r\n",
      "    output = self.call(inputs, **kwargs)\r\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/keras/layers/normalization.py\", line 198, in call\r\n",
      "    self.momentum)],\r\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\", line 1011, in moving_average_update\r\n",
      "    x, value, momentum, zero_debias=True)\r\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/moving_averages.py\", line 89, in assign_moving_average\r\n",
      "    update_delta = _zero_debias(variable, value, decay)\r\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/moving_averages.py\", line 218, in _zero_debias\r\n",
      "    trainable=False)\r\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 1328, in get_variable\r\n",
      "    constraint=constraint)\r\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 1090, in get_variable\r\n",
      "    constraint=constraint)\r\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 435, in get_variable\r\n",
      "    constraint=constraint)\r\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 404, in _true_getter\r\n",
      "    use_resource=use_resource, constraint=constraint)\r\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 796, in _get_single_variable\r\n",
      "    use_resource=use_resource)\r\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2234, in variable\r\n",
      "    use_resource=use_resource)\r\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2224, in <lambda>\r\n",
      "    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\r\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2207, in default_variable_creator\r\n",
      "    constraint=constraint)\r\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 259, in __init__\r\n",
      "    constraint=constraint)\r\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 368, in _init_from_args\r\n",
      "    initial_value(), name=\"initial_value\", dtype=dtype)\r\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 780, in <lambda>\r\n",
      "    shape.as_list(), dtype=dtype, partition_info=partition_info)\r\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/init_ops.py\", line 99, in __call__\r\n",
      "    return array_ops.zeros(shape, dtype)\r\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 1538, in zeros\r\n",
      "    output = _constant_if_small(zero, shape, dtype, name)\r\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 1496, in _constant_if_small\r\n",
      "    return constant(value, shape=shape, dtype=dtype, name=name)\r\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py\", line 202, in constant\r\n",
      "    name=name).outputs[0]\r\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\r\n",
      "    op_def=op_def)\r\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\r\n",
      "    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n",
      "\r\n",
      "ResourceExhaustedError (see above for traceback): OOM when allocating tensor of shape [] and type float\r\n",
      "\t [[Node: batch_normalization_71/moving_variance/local_step/Initializer/zeros = Const[_class=[\"loc:@batch_normalization_71/moving_variance/Assign\"], dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 0>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python yolo_video.py --image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs/000/trained_weights.h5 model, anchors, and classes loaded.\n"
     ]
    }
   ],
   "source": [
    "yolo = YOLO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'pass.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_img(yolo,path):\n",
    "    image = Image.open(path)\n",
    "    \n",
    "    r_image = yolo.detect_image(image)\n",
    "    r_image.show()\n",
    "    return r_image\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-28935580a9bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'image' is not defined"
     ]
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "fail 0.60 (20, 40) (33, 51)\n",
      "0.06785829295404255\n"
     ]
    }
   ],
   "source": [
    "r = detect_img(yolo,'VOCdevkit/VOC2007/JPEGImages/Porsche.C39WQ00XK94K_1Y20B3_007038.XRAY_Bottom_Right.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = r.resize((100,100),Image.ANTIALIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=100x100 at 0x7F41D47CD780>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
